{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- データセットのダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- データセットの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 1, 784)\n",
    "X_test = X_test.reshape(-1, 1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 784)\n",
      "(10000, 1, 784)\n",
      "uint8\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255\n",
      "  247 127   0   0   0   0   0   0   0   0   0   0   0   0  30  36  94 154\n",
      "  170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0   0   0\n",
      "    0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82\n",
      "   82  56  39   0   0   0   0   0   0   0   0   0   0   0   0  18 219 253\n",
      "  253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  35 241\n",
      "  225 160 108   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253\n",
      "  253 201  78   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0  18 171 219 253 253 253 253 195\n",
      "   80   9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0 136 253 253 253 212 135 132  16\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape) # (60000, 28, 28)\n",
    "print(X_test.shape) # (10000, 28, 28)\n",
    "print(X_train[0].dtype) # uint8\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 平滑化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = X_train.reshape(-1, 784) # Sprint11 削除\n",
    "#X_test = X_test.reshape(-1, 784) # Sprint11 削除\n",
    "\n",
    "X_train = X_train.reshape(-1, 1, 784) # Sprint11 追加\n",
    "X_test = X_test.reshape(-1, 1, 784) # Sprint11 追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprint11 追加************\n",
    "X_train = X_train[:2400]\n",
    "y_train = y_train[:2400]\n",
    "X_test = X_test[:600]\n",
    "y_test = y_test[:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2400, 1, 784), (600, 1, 784))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 画像データの可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 0\n",
    "image = X_train[index].reshape(28,28)\n",
    "# X_train[index]: (784,)\n",
    "# image: (28, 28)\n",
    "plt.imshow(image, 'gray')\n",
    "plt.title('label : {}'.format(y_train[index]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(X_train.max()) # 1.0\n",
    "print(X_train.min()) # 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400,)\n",
      "(2400, 10)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "print(y_train.shape) # (60000,)\n",
    "print(y_train_one_hot.shape) # (60000, 10)\n",
    "print(y_train_one_hot.dtype) # float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1920, 1, 784) (1920, 10)\n",
      "(480, 1, 784) (480, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "\n",
    "print(X_train.shape, y_train.shape) # (48000, 784)\n",
    "print(X_val.shape, y_val.shape) # (12000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成\n",
    "チャンネル数を1に限定した1次元畳み込み層のクラスSimpleConv1dを作成してください。基本構造は前のSprintで作成した全結合層のFCクラスと同じになります。なお、重みの初期化に関するクラスは必要に応じて作り変えてください。Xavierの初期値などを使う点は全結合層と同様です。\n",
    "\n",
    "\n",
    "ここでは パディング は考えず、ストライド も1に固定します。また、複数のデータを同時に処理することも考えなくて良く、バッチサイズは1のみに対応してください。この部分の拡張はアドバンス課題とします。\n",
    "\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。\n",
    "\n",
    "$$\n",
    "a_i = \\sum_{s=0}^{F-1}x_{(i+s)}w_s+b\n",
    "$$\n",
    "\n",
    "\n",
    "$a_i$ : 出力される配列のi番目の値\n",
    "\n",
    "\n",
    "$F$ : フィルタのサイズ\n",
    "\n",
    "\n",
    "$x_{(i+s)}$ : 入力の配列の(i+s)番目の値\n",
    "\n",
    "\n",
    "$w_s$ : 重みの配列のs番目の値\n",
    "\n",
    "\n",
    "$b$ : バイアス項\n",
    "\n",
    "\n",
    "全てスカラーです。\n",
    "\n",
    "\n",
    "次に更新式です。ここがAdaGradなどに置き換えられる点は全結合層と同様です。\n",
    "$$\n",
    "w_s^{\\prime} = w_s - \\alpha \\frac{\\partial L}{\\partial w_s} \\\\\n",
    "b^{\\prime} = b - \\alpha \\frac{\\partial L}{\\partial b}\n",
    "$$\n",
    "\n",
    "\n",
    "$\\alpha$ : 学習率\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_s}$ : $w_s$ に関する損失 $L$ の勾配\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b}$ : $b$ に関する損失 $L$ の勾配\n",
    "\n",
    "\n",
    "勾配 $\\frac{\\partial L}{\\partial w_s}$ や $\\frac{\\partial L}{\\partial b}$ を求めるためのバックプロパゲーションの数式が以下です。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_s} = \\sum_{i=0}^{N_{out}-1} \\frac{\\partial L}{\\partial a_i}x_{(i+s)}\\\\\n",
    "\\frac{\\partial L}{\\partial b} = \\sum_{i=0}^{N_{out}-1} \\frac{\\partial L}{\\partial a_i}\n",
    "$$\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial a_i}$ : 勾配の配列のi番目の値\n",
    "\n",
    "\n",
    "$N_{out}$ : 出力のサイズ\n",
    "\n",
    "\n",
    "前の層に流す誤差の数式は以下です。\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_j} = \\sum_{s=0}^{F-1} \\frac{\\partial L}{\\partial a_{(j-s)}}w_s\n",
    "$$\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x_j}$ : 前の層に流す誤差の配列のj番目の値\n",
    "\n",
    "\n",
    "ただし、 $j-s<0$ または $j-s>N_{out}-1$ のとき $\\frac{\\partial L}{\\partial a_{(j-s)}} =0$ です。\n",
    "\n",
    "\n",
    "全結合層との大きな違いは、重みが複数の特徴量に対して共有されていることです。この場合は共有されている分の誤差を全て足すことで勾配を求めます。計算グラフ上での分岐はバックプロパゲーションの際に誤差の足し算をすれば良いことになります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scratch1dCNNClassifier:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self):       \n",
    "        self.W = np.array([3,5,7]) \n",
    "        self.B = np.array([1])\n",
    "        self.padding = 0\n",
    "        self.strides = 1        \n",
    "        self.filters = len(self.W)\n",
    "        self.a = np.array([])\n",
    "        self.dW = np.array([])\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes_bf)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes_af)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.N_in = len(self.X)\n",
    "        self.N_out = int((self.N_in + 2*self.padding - self.filters) / self.strides + 1) # 出力サイズの計算\n",
    "        \n",
    "        self.a = np.append(self.a, [np.dot(self.X[i : i+self.filters], self.W) + self.B for i in range(self.N_out)])\n",
    "\n",
    "        return self.a\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.dB = np.sum(dA, axis=0)        \n",
    "        self.dW = np.append(self.dW, [np.dot(self.X[i : i+self.N_out].T, dA) for i in range(self.filters)])\n",
    "\n",
    "\n",
    "        self.dX = np.zeros(len(self.X))            \n",
    "        for j in range(len(self.X)):\n",
    "            for s in range(len(self.W)):\n",
    "                if j - s < 0 or j - s > 1:\n",
    "                    self.dX[j] = self.dX[j]\n",
    "                else:\n",
    "                    self.dX[j] = self.dX[j] + dA[j - s] * self.W[s]\n",
    "\n",
    "        return self.dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】1次元畳み込み後の出力サイズの計算\n",
    "畳み込みを行うと特徴量の数が変化します。どのように変化するかは以下の数式から求められます。パディングやストライドも含めています。この計算を行う関数を作成してください。\n",
    "\n",
    "\n",
    "$$\n",
    "N_{out} = \\frac{N_{in}+2P-F}{S}+1\n",
    "$$\n",
    "\n",
    "$N_{out}$ : 出力のサイズ（特徴量の数）\n",
    "\n",
    "\n",
    "$N_{in}$ : 入力のサイズ（特徴量の数）\n",
    "\n",
    "\n",
    "$P$ : ある方向へのパディングの数\n",
    "\n",
    "\n",
    "$F$ : フィルタのサイズ\n",
    "\n",
    "\n",
    "$S$ : ストライドのサイズ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】小さな配列での1次元畳み込み層の実験\n",
    "\n",
    "次に示す小さな配列でフォワードプロパゲーションとバックプロパゲーションが正しく行えているか確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルデータ\n",
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35., 50.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Scratch1dCNNClassifier()\n",
    "model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_a = np.array([10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 30., 110., 170., 140.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.backward(delta_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成\n",
    "\n",
    "チャンネル数を1に限定しない1次元畳み込み層のクラスConv1dを作成してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self):       \n",
    "        self.W = np.ones((3, 2, 3)) \n",
    "        self.B = np.array([1, 2, 3])\n",
    "        self.padding = 0\n",
    "        self.strides = 1        \n",
    "        self.filters = self.W.shape[2]        \n",
    "        #self.a = np.array([])\n",
    "        #self.dW = np.array([])\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes_bf)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes_af)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.N_in = self.X.shape[1]\n",
    "        self.N_out = int((self.N_in + 2*self.padding - self.filters) / self.strides + 1) # 出力サイズの計算\n",
    "\n",
    "        self.a = np.zeros((self.W.shape[0], self.N_out)) #出力チャネル数、出力サイズ\n",
    "        for i in range(self.W.shape[0]): #出力チャネル\n",
    "            for j in range(self.X.shape[0]): #入力チャネル\n",
    "                for s in range(self.N_out): #出力サイズ\n",
    "                    self.a[i, j] = self.a[i, j] + np.dot(self.X[j, s: s+self.filters], self.W[i, j])\n",
    "                    \n",
    "        #バイアス\n",
    "        self.a = self.a + self.B.reshape(-1, 1)\n",
    "\n",
    "        return self.a\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.dB = np.sum(dA, axis=1)\n",
    "        \n",
    "        self.dW = np.zeros((self.W.shape[0], self.X.shape[0], self.filters)) #出力チャネル数、出力サイズ、フィルタサイズ\n",
    "        for i in range(self.W.shape[0]): #出力チャネル\n",
    "            for j in range(self.X.shape[0]): #入力チャネル\n",
    "                for s in range(self.filters): #フィルタサイズ\n",
    "                        self.dW[i, j, s] = np.dot(self.X[j][s : s+self.N_out].T, dA[s])\n",
    "\n",
    "        self.dX = np.zeros(self.X.shape)\n",
    "        for i in range(self.W.shape[0]): #出力チャネル\n",
    "            for j in range(self.X.shape[0]): #入力チャネル\n",
    "                for s in range(self.filters): #フィルタサイズ\n",
    "                    for n in range(self.N_out): #出力サイズ\n",
    "                        self.dX[j, s + n] = self.dX[j, s + n] + dA[i, n] * self.W[i, j, s]\n",
    "\n",
    "        return self.dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conv1d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。\n",
    "w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\n",
    "b = np.array([1, 2, 3]) # （出力チャンネル数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 4])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[0], x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16., 22.],\n",
       "       [17., 23.],\n",
       "       [18., 24.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_a = np.array([[10, 20], [10, 20], [10, 20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 20])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_a[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30., 90., 90., 60.],\n",
       "       [30., 90., 90., 60.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.backward(delta_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.ones((3, 2, 3)) \n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape[0], test.shape[1], test.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16, 22],\n",
       "       [17, 23],\n",
       "       [18, 24]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[16, 22], [17, 23], [18, 24]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = np.array([1, 2, 3])\n",
    "B.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]])\n",
    "x[0][0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題5】（アドバンス課題）パディングの実装\n",
    "畳み込み層にパディングの機能を加えてください。1次元配列の場合、前後にn個特徴量を増やせるようにしてください。\n",
    "\n",
    "\n",
    "最も単純なパディングは全て0で埋める ゼロパディング であり、CNNでは一般的です。他に端の値を繰り返す方法などもあります。\n",
    "\n",
    "\n",
    "フレームワークによっては、元の入力のサイズを保つようにという指定をすることができます。この機能も持たせておくと便利です。なお、NumPyにはパディングの関数が存在します。\n",
    "\n",
    "\n",
    "[numpy.pad — NumPy v1.17 Manual](https://numpy.org/doc/stable/reference/generated/numpy.pad.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題6】（アドバンス課題）ミニバッチへの対応\n",
    "\n",
    "ここまでの課題はバッチサイズ1で良いとしてきました。しかし、実際は全結合層同様にミニバッチ学習が行われます。Conv1dクラスを複数のデータが同時に計算できるように変更してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題7】（アドバンス課題）任意のストライド数\n",
    "\n",
    "ストライドは1限定の実装をしてきましたが、任意のストライド数に対応できるようにしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題8】学習と推定\n",
    "\n",
    "これまで使ってきたニューラルネットワークの全結合層の一部をConv1dに置き換えてMNISTを学習・推定し、Accuracyを計算してください。\n",
    "\n",
    "\n",
    "出力層だけは全結合層をそのまま使ってください。ただし、チャンネルが複数ある状態では全結合層への入力は行えません。その段階でのチャンネルは1になるようにするか、 平滑化 を行なってください。\n",
    "\n",
    "\n",
    "画像に対しての1次元畳み込みは実用上は行わないことのため、精度は問いません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scratch1dCNNClassifier:\n",
    "    \"\"\"\n",
    "    ディープニューラルネットワーク分類器\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    activaiton : {'sigmoid', 'tanh', 'relu'} \n",
    "        活性化関数の種類\n",
    "    n_nodes : list\n",
    "        ノードの構成 例 [400, 200, 100]\n",
    "    n_output : int\n",
    "        出力層の数\n",
    "    alpha : float\n",
    "        学習率\n",
    "    optimizer :  {'sgd', 'adagrad'}\n",
    "        最適化手法の種類\n",
    "    filter_num : int\n",
    "        フィルタ数\n",
    "    filter_size : int\n",
    "        フィルタサイズ\n",
    "        \n",
    "    Attributes\n",
    "    -------------\n",
    "    FC[n_layers] :  dict\n",
    "        結合層を管理する辞書\n",
    "    activation : dict\n",
    "        活性化関数を管理する辞書\n",
    "    self.epochs : int\n",
    "        エポック数(初期値：10)\n",
    "    self.batch_size : int\n",
    "        バッチサイズ(初期値：20)\n",
    "    self.n_features : int\n",
    "        特徴量の数\n",
    "    self.val_is_true : boolean\n",
    "        検証用データの有無    \n",
    "    self.loss : 空のndarray\n",
    "        訓練データに対する損失の記録\n",
    "    self.loss_val : 空のndarray\n",
    "        検証データに対する損失の記録\n",
    "        \n",
    "    \"\"\"    \n",
    "    def __init__(self, activation, n_nodes, n_output, lr, optimizer, filter_num, filter_size):\n",
    "        self.select_activation = activation\n",
    "        self.n_nodes = n_nodes\n",
    "        self.n_output = n_output\n",
    "        self.lr = lr\n",
    "        self.select_optimizer = optimizer\n",
    "        \n",
    "        # Sprint11 追加*******************************\n",
    "        self.filter_num  = filter_num                         # フィルタ数\n",
    "        self.filter_size   = filter_size    # フィルタサイズ\n",
    "        # Sprint11 追加*******************************\n",
    "            \n",
    "    def __initialize_n_layers(self):\n",
    "        \"\"\"\n",
    "        N層を初期化する。\n",
    "        sigmoid関数とtanh関数が活性化関数の場合：Xavierを初期値\n",
    "        ReLU関数が活性化関数の場合：Heを初期値\n",
    "        \"\"\"\n",
    "        self.activation = dict()\n",
    "        self.FC = dict()\n",
    "        # Sprint11 追加*********************************************************************\n",
    "        #n_nodes = self.out // (2**(1))\n",
    "        if self.select_activation == 'sigmoid':\n",
    "            self.FC[0] = FC(self.out, self.n_nodes[0], \n",
    "                            XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
    "            self.activation[0] = Sigmoid()\n",
    "        elif self.select_activation == 'tanh':\n",
    "            self.FC[0] = FC(self.out, self.n_nodes[0], \n",
    "                            XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
    "            self.activation[0] = Tanh()\n",
    "        elif self.select_activation == 'relu':\n",
    "            self.FC[0] = FC(self.out, self.n_nodes[0], \n",
    "                            HeInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
    "            self.activation[0] = ReLU()\n",
    "        # Sprint11 追加*********************************************************************\n",
    "\n",
    "        for n_layer in range(len(self.n_nodes)):            \n",
    "            if n_layer == len(self.n_nodes) -1:\n",
    "                if self.select_activation == 'sigmoid':\n",
    "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_output, \n",
    "                                              XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
    "                    self.activation[n_layer + 1] = Softmax()\n",
    "                elif self.select_activation == 'tanh':\n",
    "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_output,\n",
    "                                              XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
    "                    self.activation[n_layer + 1] = Softmax()\n",
    "                elif self.select_activation == 'relu':\n",
    "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_output,\n",
    "                                              HeInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
    "                    self.activation[n_layer + 1] = Softmax()\n",
    "            else:\n",
    "                if self.select_activation == 'sigmoid':\n",
    "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_nodes[n_layer+1], \n",
    "                                              XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
    "                    self.activation[n_layer + 1] = Sigmoid()\n",
    "                elif self.select_activation == 'tanh':\n",
    "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_nodes[n_layer+1],\n",
    "                                              XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
    "                    self.activation[n_layer + 1] = Tanh()\n",
    "                elif self.select_activation == 'relu':\n",
    "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_nodes[n_layer+1],\n",
    "                                              HeInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
    "                    self.activation[n_layer + 1] = ReLU()\n",
    "    \n",
    "    def fit(self, X, y, epochs=10, batch_size=20):  \n",
    "        self.epochs = epochs                            # エポック数     \n",
    "        self.batch_size = batch_size               # バッチサイズ\n",
    "        self.loss = np.zeros(self.epochs)        # 学習曲線・目的関数の出力用(訓練データ)\n",
    "        self.loss_val = np.zeros(self.epochs) # 学習曲線・目的関数の出力用(検証データ)        \n",
    "        \n",
    "        if self.select_optimizer == 'sgd':\n",
    "            self.optimizer = SGD(self.lr)\n",
    "        elif self.select_optimizer == 'adagrad':\n",
    "            self.optimizer = AdaGrad(self.lr)            \n",
    "        \n",
    "        # Sprint11 追加*********************************************************************\n",
    "        # 畳み込み層クラス\n",
    "        self.input_channel = X.shape[1]\n",
    "        self.input_feature = X.shape[2]\n",
    "        \n",
    "        self.conv1d = Conv1d(self.select_activation, self.optimizer, self.filter_num, self.input_channel, self.filter_size)\n",
    "        if self.select_activation == 'sigmoid':\n",
    "            self.activation_conv = Sigmoid()\n",
    "        elif self.select_activation == 'tanh':\n",
    "            self.activation_conv = Tanh()\n",
    "        elif self.select_activation == 'relu':\n",
    "            self.activation_conv = ReLU()\n",
    " \n",
    "        #ミニバッチの取得\n",
    "        get_mini_batch = GetMiniBatch(X, y, self.batch_size)\n",
    "\n",
    "        # 平滑化クラス\n",
    "        self.flatten = Flatten()\n",
    "        self.out = self.filter_num * (self.input_feature - (self.filter_size - 1))\n",
    "        # Sprint11 追加*********************************************************************        \n",
    "        \n",
    "        self.__initialize_n_layers()\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            #get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size) # Sprint11 削除\n",
    "            print(\"epoch \", epoch + 1 , \" processing . . .\")\n",
    "            for mini_X_train,  mini_y_train in get_mini_batch:\n",
    "                self.X_ = mini_X_train\n",
    "                self.y_ = mini_y_train\n",
    "                \n",
    "                # フォワードプロバゲーション\n",
    "                # Sprint11 追加*********************************************************************\n",
    "                #畳み込み(1層目)\n",
    "                self.A = self.conv1d.forward(self.X_) \n",
    "                self.Z = self.activation_conv.forward(self.A)    \n",
    "                \n",
    "                #平滑化\n",
    "                self.F = self.flatten.forward(self.Z)\n",
    "                               \n",
    "                self.A = self.FC[0].forward(self.F)\n",
    "                self.Z = self.activation[0].forward(self.A)                         \n",
    "                \n",
    "                # Sprint11 追加*********************************************************************\n",
    "                \n",
    "                self.A = self.FC[1].forward(self.Z)            #1層目\n",
    "                self.Z = self.activation[1].forward(self.A) #1層目\n",
    "                for n_layer in range(2, len(self.n_nodes) + 1): #2層目以降\n",
    "                    self.A = self.FC[n_layer].forward(self.Z)\n",
    "                    self.Z = self.activation[n_layer].forward(self.A)\n",
    "                \n",
    "                # バックプロパゲーション\n",
    "                self.dA, self.loss[epoch] = self.activation[len(self.n_nodes)].backward(self.Z, self.y_) #最終層, 交差エントロピー誤差\n",
    "                self.dZ = self.FC[len(self.n_nodes)].backward(self.dA) #最終層           \n",
    "                for n_layer in reversed(range(0, len(self.n_nodes))): #最終層 - 1\n",
    "                    self.dA = self.activation[n_layer].backward(self.dZ)\n",
    "                    self.dZ = self.FC[n_layer].backward(self.dA)\n",
    "\n",
    "                # Sprint11 追加********************************\n",
    "                # shapeの戻入\n",
    "                self.dF = self.flatten.backward(self.dZ)\n",
    "                \n",
    "                # 畳み込み層\n",
    "                self.dA = self.activation_conv.backward(self.dF)\n",
    "                self.dZ = self.conv1d.backward(self.dA)\n",
    "                # Sprint11 追加********************************\n",
    "                \n",
    "    def predict(self,X):\n",
    "        \n",
    "        # Sprint11 追加********************************\n",
    "        #畳み込み(1層目)\n",
    "        self.A = self.conv1d.forward(X) \n",
    "        self.Z = self.activation_conv.forward(self.A)    \n",
    "\n",
    "        #平滑化\n",
    "        self.F = self.flatten.forward(self.Z)\n",
    "\n",
    "        self.A = self.FC[0].forward(self.F)            #1層目\n",
    "        self.Z = self.activation[0].forward(self.A) #1層目\n",
    "        # Sprint11 追加********************************\n",
    "        \n",
    "        # フォワードプロバゲーション\n",
    "        self.A = self.FC[1].forward(self.Z)                     #1層目\n",
    "        self.Z = self.activation[1].forward(self.A) #1層目\n",
    "        for n_layer in range(2, len(self.n_nodes) + 1): #2層目以降\n",
    "            self.A = self.FC[n_layer].forward(self.Z)\n",
    "            self.Z = self.activation[n_layer].forward(self.A)\n",
    "        \n",
    "        return np.argmax(self.Z, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, activation, optimizer, filter_num, input_channel, filter_size):\n",
    "        self.activation = activation\n",
    "        self.optimizer = optimizer\n",
    "        self.filter_num = filter_num\n",
    "        self.input_channel = input_channel\n",
    "        self.filter_size = filter_size\n",
    "        self.pad = 0\n",
    "        self.stride = 1\n",
    "        \n",
    "        if self.activation == 'sigmoid':\n",
    "            initializer = XavierInitializer(self.filter_num, self.input_channel, self.filter_size)\n",
    "            self.W = initializer.W(_, _)\n",
    "            self.B = initializer.B(_)            \n",
    "        elif self.activation == 'tanh':\n",
    "            initializer = XavierInitializer(self.filter_num, self.input_channel, self.filter_size)\n",
    "            self.W = initializer.W(_, _)\n",
    "            self.B = initializer.B(_)          \n",
    "        elif self.activation == 'relu':\n",
    "            initializer = HeInitializer(self.filter_num, self.input_channel, self.filter_size)\n",
    "            self.W = initializer.W(_, _)\n",
    "            self.B = initializer.B(_)\n",
    "\n",
    "        self.X = None\n",
    "        self.N_in = None\n",
    "        self.N_out = None\n",
    "        self.a = None\n",
    "        self.dB = None\n",
    "        self.dW = None\n",
    "        self.dX = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes_bf)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes_af)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        \n",
    "        #shapeの取得\n",
    "        batch_size, input_channel, self.N_in = self.X.shape #(バッチサイズ, 入力チャンネル, 特徴量)\n",
    "        FN, C, FS = self.W.shape #(出力チャンネル,  入力チャンネル, フィルタサイズ)\n",
    "        \n",
    "        self.N_out = int((self.N_in + 2*self.pad - self.filter_size) / self.stride + 1) # 出力サイズの計算\n",
    "\n",
    "        self.a = np.zeros((batch_size, FN, self.N_out)) #バッチサイズ、出力チャネル数、出力サイズ\n",
    "        for b in range(batch_size): # バッチサイズ\n",
    "            for i in range(FN): #出力チャネル\n",
    "                for j in range(C): #入力チャネル\n",
    "                    for s in range(self.N_out): #出力サイズ\n",
    "                        self.a[b, i, s] = self.a[b, i, s] + np.sum(self.X[b, j, s: s+self.filter_size] * self.W[i, j, :])\n",
    "                        \n",
    "        #バイアス\n",
    "        self.a = self.a + self.B.reshape(1, -1, 1)\n",
    "\n",
    "        return self.a\n",
    "       \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        batch, C, W = self.X.shape # (バッチサイズ、入力チャンネル数、特徴量数)\n",
    "        FN, C, FS = self.W.shape # (出力チャンネル数、入力チャンネル数、フィルタサイズ)\n",
    "        \n",
    "        #空の配列\n",
    "        self.dW = np.zeros(self.W.shape) #Wの勾配\n",
    "        self.dX = np.zeros(self.X.shape) #Xの勾配\n",
    "\n",
    "        self.dB = np.sum(dA, axis=2)\n",
    "        \n",
    "        for b in range(batch): # バッチサイズ\n",
    "            for i in range(FN): #出力チャネル\n",
    "                for j in range(C): #入力チャネル\n",
    "                    for s in range(FS): #フィルタサイズ\n",
    "                        for x in range(self.N_out): #特徴量\n",
    "                            self.dW[i, j, s] = self.dW[i, j, s] + dA[b, i, x] * self.X[b, j, s + x]\n",
    "                            self.dX[b, j, s + x] = self.dX[b, j, s + x] + dA[b, i, x] * self.W[i, j, s]\n",
    "\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return self.dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten():\n",
    "    def __init__(self):\n",
    "        self.X_shape = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # 1次元化\n",
    "        X_1d = X.reshape(X.shape[0], -1)\n",
    "        \n",
    "        # shapeの記録\n",
    "        self.X_shape = X.shape\n",
    "        \n",
    "        return X_1d    \n",
    "\n",
    "    def backward(self, X):\n",
    "        # shapeの返戻\n",
    "        X = X.reshape(self.X_shape)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"        \n",
    "        self.X = X\n",
    "        \n",
    "        return np.dot(self.X, self.W) + self.B\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.dB = dA\n",
    "        self.dW = np.dot(self.X.T, dA)\n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    Xavierのクラス\n",
    "    \"\"\"\n",
    "    # Sprint11 追加*******************************\n",
    "    def __init__(self, filter_num=None, input_channel=None, filter_size=None):        \n",
    "        self.filter_num = filter_num\n",
    "        self.input_channel = input_channel\n",
    "        self.filter_size = filter_size\n",
    "    # Sprint11 追加*******************************  \n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : 次の形のndarray, shape (n_nodes1, n_nodes2)\n",
    "            重み\n",
    "        \"\"\"\n",
    "        # Sprint11 追加*******************************        \n",
    "        if self.filter_num and self.input_channel and self.filter_size is not None: # 畳み込み層\n",
    "            W = np.random.randn(self.filter_num, self.input_channel, self.filter_size)\n",
    "        else: # 全結合層\n",
    "        # Sprint11 追加*******************************\n",
    "            W = np.random.randn(n_nodes1, n_nodes2) / np.sqrt(n_nodes1)       \n",
    "        \n",
    "        return W\n",
    "        \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B : 次の形のndarray, shape (n_nodes2, )\n",
    "            バイアス\n",
    "        \"\"\"\n",
    "        # Sprint11 追加*******************************        \n",
    "        if self.filter_num and self.input_channel and self.filter_size is not None: # 畳み込み層\n",
    "            B = np.random.randn(self.filter_num)\n",
    "        else: # 全結合層\n",
    "        # Sprint11 追加*******************************\n",
    "            B = np.zeros(n_nodes2)\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    Heのクラス\n",
    "    \"\"\"\n",
    "    # Sprint11 追加*******************************\n",
    "    def __init__(self, filter_num=None, input_channel=None, filter_size=None):        \n",
    "        self.filter_num = filter_num\n",
    "        self.input_channel = input_channel\n",
    "        self.filter_size = filter_size\n",
    "    # Sprint11 追加*******************************        \n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : 次の形のndarray, shape (n_nodes1, n_nodes2)\n",
    "            重み\n",
    "        \"\"\"\n",
    "        # Sprint11 追加*******************************        \n",
    "        if self.filter_num and self.input_channel and self.filter_size is not None: # 畳み込み層\n",
    "            W = np.random.randn(self.filter_num, self.input_channel, self.filter_size) * np.sqrt(2 / self.filter_num) \n",
    "        else: # 全結合層\n",
    "        # Sprint11 追加*******************************\n",
    "            W = np.random.randn(n_nodes1, n_nodes2) * np.sqrt(2 / n_nodes1)\n",
    "    \n",
    "        return W\n",
    "        \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B : 次の形のndarray, shape (n_nodes2, )\n",
    "            バイアス\n",
    "        \"\"\"\n",
    "        # Sprint11 追加*******************************        \n",
    "        if self.filter_num and self.input_channel and self.filter_size is not None: # 畳み込み層\n",
    "            B = np.random.randn(self.filter_num)\n",
    "        else: # 全結合層\n",
    "        # Sprint11 追加*******************************\n",
    "            B = np.random.randn(n_nodes2)\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.W = layer.W - self.lr * layer.dW\n",
    "        layer.B = layer.B - self.lr * layer.dB.mean(axis=0)\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    AdaGradのクラス\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : 学習率\n",
    "    \n",
    "    Attributes\n",
    "    -------------\n",
    "    lr : 学習率\n",
    "    HW : int(初期値), ndarray\n",
    "    HB :  int(初期値), ndarray\n",
    "    \"\"\"    \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.HW= 0 # 初期値：0\n",
    "        self.HB = 0 # 初期値：0       \n",
    "\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : インスタンス\n",
    "            更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : インスタンス\n",
    "            更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        #初期化\n",
    "        self.HW = np.zeros_like(layer.W)\n",
    "        self.HB = np.zeros_like(layer.B)\n",
    "        \n",
    "        #更新\n",
    "        self.HW = self.HW + (layer.dW**2) #/ layer.dB.shape[0]\n",
    "        self.HB = self.HB + (layer.dB**2).mean(axis=0)\n",
    "        layer.W = layer.W - self.lr * 1 / np.sqrt(self.HW + 1e-7) * layer.dW #/ layer.dB.shape[0]\n",
    "        layer.B = layer.B - self.lr * 1 / np.sqrt(self.HB + 1e-7) * layer.dB.mean(axis=0)\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        \n",
    "        self.Z = 1.0 / (1.0 + np.exp(-self.A))\n",
    "        \n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "\n",
    "        return dZ * (1 - self.Z) * self.Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        \n",
    "        return np.tanh(self.A)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \n",
    "        return dZ * (1.0 - (np.tanh(self.A) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, A):    \n",
    "        self.A = A\n",
    "      \n",
    "        return np.maximum(self.A, 0)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "            \n",
    "        return np.where(self.A > 0, dZ, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \n",
    "        return np.exp(A) / np.sum(np.exp(A), axis=1, keepdims=True)\n",
    "    \n",
    "    def backward(self, Z, y):\n",
    "        \n",
    "        dA = Z - y\n",
    "        \n",
    "        # 交差エントロピー誤差\n",
    "        batch_size = y.shape[0]\n",
    "        loss = -np.sum(y * np.log(Z)) / batch_size\n",
    "\n",
    "        return dA, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1  processing . . .\n",
      "epoch  2  processing . . .\n",
      "epoch  3  processing . . .\n",
      "epoch  4  processing . . .\n",
      "epoch  5  processing . . .\n",
      "epoch  6  processing . . .\n",
      "epoch  7  processing . . .\n",
      "epoch  8  processing . . .\n",
      "epoch  9  processing . . .\n",
      "epoch  10  processing . . .\n",
      "epoch  11  processing . . .\n",
      "epoch  12  processing . . .\n",
      "epoch  13  processing . . .\n",
      "epoch  14  processing . . .\n",
      "epoch  15  processing . . .\n"
     ]
    }
   ],
   "source": [
    "model = Scratch1dCNNClassifier(activation='tanh', n_nodes=[400, 200, 100], n_output=10, lr=0.001, optimizer='sgd', filter_num=3, filter_size=3)\n",
    "model.fit(X_train, y_train, epochs=15, batch_size=10)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, 0, 4, 1, 4, 9, 6, 9, 0, 6, 9, 0, 1, 5, 9, 7, 2, 4, 9, 6,\n",
       "       6, 5, 4, 0, 7, 4, 0, 1, 3, 1, 3, 6, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2,\n",
       "       3, 5, 1, 2, 4, 4, 6, 3, 5, 5, 2, 0, 4, 1, 9, 5, 7, 8, 9, 2, 7, 4,\n",
       "       2, 4, 3, 0, 7, 0, 2, 9, 1, 7, 3, 7, 9, 7, 9, 6, 2, 7, 8, 4, 7, 5,\n",
       "       6, 1, 3, 6, 9, 3, 1, 4, 1, 7, 6, 9, 6, 0, 5, 4, 9, 9, 2, 1, 9, 4,\n",
       "       8, 7, 3, 9, 7, 9, 4, 4, 9, 9, 5, 4, 7, 6, 9, 9, 2, 5, 8, 5, 6, 6,\n",
       "       5, 7, 8, 1, 0, 1, 6, 9, 6, 7, 3, 1, 7, 1, 8, 2, 0, 4, 9, 2, 5, 5,\n",
       "       1, 5, 6, 0, 2, 4, 4, 6, 5, 4, 6, 5, 4, 4, 1, 4, 4, 7, 2, 3, 2, 7,\n",
       "       1, 8, 1, 8, 1, 8, 5, 0, 8, 9, 2, 5, 0, 1, 1, 1, 0, 9, 0, 2, 1, 6,\n",
       "       4, 2, 3, 6, 1, 1, 1, 3, 9, 5, 2, 9, 4, 7, 9, 3, 9, 0, 3, 5, 7, 5,\n",
       "       7, 2, 2, 7, 1, 2, 8, 4, 1, 7, 3, 3, 8, 9, 7, 9, 2, 2, 4, 1, 5, 8,\n",
       "       8, 7, 2, 5, 0, 2, 4, 2, 4, 1, 9, 5, 7, 7, 2, 2, 2, 6, 8, 1, 7, 7,\n",
       "       9, 1, 8, 1, 8, 0, 3, 0, 1, 9, 9, 4, 1, 8, 2, 1, 2, 9, 7, 5, 9, 2,\n",
       "       6, 4, 1, 5, 4, 2, 9, 2, 0, 4, 0, 0, 2, 8, 6, 7, 1, 2, 4, 0, 2, 7,\n",
       "       4, 3, 3, 0, 0, 5, 1, 9, 6, 5, 2, 6, 7, 7, 9, 3, 7, 9, 2, 0, 7, 1,\n",
       "       1, 2, 1, 5, 3, 2, 9, 7, 8, 6, 3, 4, 1, 3, 8, 1, 0, 5, 1, 7, 1, 5,\n",
       "       0, 6, 1, 8, 5, 1, 9, 9, 4, 6, 7, 2, 5, 0, 6, 5, 6, 3, 7, 2, 2, 8,\n",
       "       8, 5, 9, 1, 1, 4, 0, 7, 3, 7, 6, 1, 6, 2, 1, 9, 2, 8, 6, 1, 9, 5,\n",
       "       2, 5, 4, 4, 2, 8, 3, 9, 2, 4, 0, 0, 3, 1, 7, 7, 3, 7, 9, 7, 1, 9,\n",
       "       2, 1, 9, 2, 9, 2, 0, 4, 9, 1, 4, 8, 1, 8, 4, 4, 9, 7, 8, 3, 7, 6,\n",
       "       0, 0, 3, 0, 8, 0, 6, 4, 8, 5, 3, 3, 2, 2, 9, 1, 2, 6, 8, 0, 9, 6,\n",
       "       6, 6, 7, 8, 8, 2, 2, 8, 8, 9, 6, 1, 8, 4, 1, 2, 2, 9, 1, 9, 7, 7,\n",
       "       4, 0, 9, 9, 9, 1, 0, 5, 2, 3, 7, 2, 9, 4, 0, 6, 3, 9, 3, 2, 1, 3,\n",
       "       1, 5, 6, 5, 7, 9, 2, 2, 2, 8, 2, 6, 5, 4, 1, 9, 7, 1, 5, 0, 3, 8,\n",
       "       2, 1, 9, 6, 9, 4, 6, 4, 2, 1, 8, 2, 5, 4, 4, 4, 4, 0, 0, 2, 3, 2,\n",
       "       7, 1, 0, 6, 7, 4, 4, 7, 9, 6, 9, 0, 9, 8, 0, 9, 6, 0, 6, 4, 5, 4,\n",
       "       9, 3, 3, 9, 3, 3, 2, 7, 8, 0, 2, 2, 1, 7, 0, 6, 9, 4, 3, 2, 0, 9,\n",
       "       6, 5, 8, 0, 9, 9])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.855\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94        53\n",
      "           1       0.96      1.00      0.98        73\n",
      "           2       0.73      0.91      0.81        64\n",
      "           3       0.93      0.66      0.77        62\n",
      "           4       0.88      0.84      0.85        67\n",
      "           5       0.83      0.70      0.76        56\n",
      "           6       0.88      0.88      0.88        52\n",
      "           7       0.81      0.91      0.86        57\n",
      "           8       0.88      0.73      0.80        52\n",
      "           9       0.77      0.94      0.85        64\n",
      "\n",
      "    accuracy                           0.85       600\n",
      "   macro avg       0.86      0.85      0.85       600\n",
      "weighted avg       0.86      0.85      0.85       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy : {}\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
