{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下の論文を読み問題に答えてください。CNNを使った物体検出（Object Detection）の代表的な研究です。\n",
    "\n",
    "\n",
    "[8]Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. In: Advances in neural information processing systems. (2015) 91–99\n",
    "\n",
    "\n",
    "(https://arxiv.org/pdf/1506.01497.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 物体検出の分野にはどういった手法が存在したか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">SPPnet やFast R-CNNといった手法が存在する。\n",
    ">\n",
    ">SPPnet：R-CNN(CVPR 2014)では、固定サイズの画像を入力として識別していましたが、 SPPnet(ECCV 2014)では、Spatial Pyramid Pooling (SPP)という手法を用いることで、 CNNで畳み込んだ最終層のfeature mapsを縦横可変サイズで取り扱える。\n",
    ">\n",
    ">ページ1 Abstractの２行目\\\n",
    ">Advances like __SPPnet [1] and Fast R-CNN [2]__ have reduced the running time of these detection networks,...\n",
    ">\n",
    ">他の参考論文: \n",
    ">[Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition](https://arxiv.org/pdf/1406.4729.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Fasterとあるが、どういった仕組みで高速化したのか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">最新のFast R-CNN は，領域候補に費やされた時間を無視して，非常に深いネットワークを用いてリアルタイムに近いレートを達成している。\n",
    ">\n",
    ">ページ1 INTRODUCTIONの左側 第1段落\\\n",
    ">The latest incarnation, Fast R-CNN [2], achieves near real-time rates using very deep networks [3], when ignoring the time spent on region proposals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">本論文では、深層畳み込みニューラルネットワークを用いたアルゴリズム変更計算の提案を示し、\\\n",
    ">検出ネットワークの計算量を考慮すると，提案計算がほぼコストフリーになるエレガントで効果的な解決法を示す。\n",
    ">\n",
    ">ページ1 INTRODUCTIONの右側\\\n",
    ">In this paper, we show that an algorithmic change—　computing proposals with a deep convolutional neural network—\n",
    "leads to an elegant and effective solution　where proposal computation is nearly cost-free given　the detection network’s computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) One-Stageの手法とTwo-Stageの手法はどう違うのか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">ニューラルネットワークを用いた物体検出手法は、検出を１段階で行うか２段階で行うかによって、それぞれOne-Stage DetectionとTwo-Stage Detectionに分類される。\n",
    ">One-Stage DetectionはBoundingBox生成とクラス予測を一度に行う。One-Stage Detectionは構成がシンプルになり、学習や推論の高速化が見込める。検出精度はTwo-Stage Detectionに劣る傾向がある。\n",
    ">Two-Stage Detectionでは、まず1段階目でBoundingBoxの候補を抽出する。そして2段階目でクラス予測とBoundingBoxの修正を行い、最終的な予測結果を生成する。\n",
    ">\n",
    ">ページ10 左側上段(One-Stage Detection vs. Two-Stage Proposal + Detection.)\\\n",
    ">The OverFeat paper [9] proposes a detection method that uses regressors and classifiers on sliding windows over convolutional feature maps. OverFeat is a one-stage, class-specific detection pipeline, and ours is a two-stage cascade consisting of class-agnostic proposals and class-specific detections. \n",
    ">\n",
    ">\n",
    ">他の参考論文: \n",
    ">[OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks](https://arxiv.org/pdf/1312.6229.pdf)\n",
    ">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) RPNとは何か。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">RPN は完全畳み込みネットワーク (FCN) [7] の一種であり，検出候補を生成するタスクのためにエンドツーエンドで特別に学習することができる。\n",
    ">\n",
    ">ページ1 INTRODUCTIONの右側\\\n",
    ">The RPN is thus a kind of fully convolutional network (FCN) [7] and can be trained end-toend specifically for the task for generating detection\n",
    "proposals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">RPN は，幅広いスケールやアスペクト比の領域提案を効率的に予測するように設計されている．\n",
    ">画像のピラミッド（図1、a）やフィルタのピラミッド（図1、b）を用いる一般的な手法[8], [9], [1], [2]とは対照的に、RPNは、画像のピラミッド（図1、a）やフィルタのピラ>ミッド（図1、b）を用いた予測手法である。\n",
    ">\n",
    ">ページ2 INTRODUCTIONの左側\\\n",
    ">RPNs are designed to efficiently predict region proposals with a wide range of scales and aspect ratios. \n",
    "In contrast to prevalent methods [8], [9], [1], [2] that use pyramids of images (Figure 1, a) or pyramids of filters (Figure 1, b), \n",
    "we introduce novel “anchor” boxes that serve as references at multiple scales and aspect ratios. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) RoIプーリングとは何か。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">RoI Poolingは分類を行う層への入力を固定次元にする役割がある。\\\n",
    ">物体候補の領域に対応する特徴マップをRoI Poolingにより切り出すことで特徴抽出を共通化し高速化＋特徴マップを作成するCNNにもbackprop可\n",
    ">\n",
    ">ページ5 右側上部\\\n",
    ">Nevertheless, our method achieves bounding-box regression by a different manner from previous RoIbased (Region of Interest) methods [1], [2]. In [1], [2], bounding-box regression is performed on features pooled from arbitrarily sized RoIs, and the regression weights are shared by all region sizes. \n",
    ">\n",
    ">ページ6 左側中段\\\n",
    ">The RoI pooling layer [2] in Fast R-CNN accepts the convolutional features and also the predicted bounding boxes as input, so a theoretically valid backpropagation solver should also involve gradients w.r.t. the box coordinates. These gradients are ignored in the above approximate joint training. \n",
    "In a non-approximate joint training solution, we need an RoI pooling layer that is differentiable w.r.t. the box coordinates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) Anchorのサイズはどうするのが適切か。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Anchorは物体の領域を示している矩形で、画像内に一定間隔で生成されLoss Functionで利用される。\n",
    ">論文の実験では以下 3 x 3 = 9 パターンが使われている。\n",
    ">\n",
    "> - スケール: $128^2$, $256^2$, $512^2$\n",
    "> - アスペクト比: 1:1, 1:2, 2:1\n",
    ">\n",
    ">Anchorはドメイン次第でチューニングするのが良いものと考えられる。\n",
    ">\n",
    ">ページ6 右側中段\\\n",
    ">For anchors, we use 3 scales with box areas of $128^2$, $256^2$ , and $512^2$ pixels, and 3 aspect ratios of 1:1, 1:2, and 2:1. \n",
    "These hyper-parameters are not carefully chosen for a particular dataset, and we provide ablation experiments on their effects in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">使用したデータセット：MS COCO dataset\\\n",
    ">この論文のFast R-CNNのベースラインは、テスト-devセットで39.3% mAP@0.5 である。\\\n",
    ">先行研究の  [2](Fast R-CNN)では、テスト-devセットで35.9% mAP@0.5 である。\n",
    ">\n",
    ">ページ11 上段 Table11\\\n",
    ">Our Fast R-CNN baseline has 39.3% mAP@0.5 on the test-dev set, higher than that reported in [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(8) （アドバンス課題）Faster R-CNNよりも新しい物体検出の論文では、Faster R-CNNがどう引用されているか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">回答なし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
