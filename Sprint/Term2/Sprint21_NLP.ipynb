{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-10-01 09:34:54--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "ai.stanford.edu (ai.stanford.edu) をDNSに問いあわせています... 171.64.68.10\n",
      "ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80 に接続しています... 接続しました。\n",
      "HTTP による接続要求を送信しました、応答を待っています... 200 OK\n",
      "長さ: 84125825 (80M) [application/x-gzip]\n",
      "`aclImdb_v1.tar.gz' に保存中\n",
      "\n",
      "aclImdb_v1.tar.gz   100%[===================>]  80.23M  4.66MB/s 時間 21s        \n",
      "\n",
      "2020-10-01 09:35:15 (3.87 MB/s) - `aclImdb_v1.tar.gz' へ保存完了 [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# IMDBをカレントフォルダにダウンロード\n",
    "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解凍\n",
    "!tar zxf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aclImdb/train/unsupはラベル無しのため削除\n",
    "!rm -rf aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Movie Review Dataset v1.0\r\n",
      "\r\n",
      "Overview\r\n",
      "\r\n",
      "This dataset contains movie reviews along with their associated binary\r\n",
      "sentiment polarity labels. It is intended to serve as a benchmark for\r\n",
      "sentiment classification. This document outlines how the dataset was\r\n",
      "gathered, and how to use the files provided. \r\n",
      "\r\n",
      "Dataset \r\n",
      "\r\n",
      "The core dataset contains 50,000 reviews split evenly into 25k train\r\n",
      "and 25k test sets. The overall distribution of labels is balanced (25k\r\n",
      "pos and 25k neg). We also include an additional 50,000 unlabeled\r\n",
      "documents for unsupervised learning. \r\n",
      "\r\n",
      "In the entire collection, no more than 30 reviews are allowed for any\r\n",
      "given movie because reviews for the same movie tend to have correlated\r\n",
      "ratings. Further, the train and test sets contain a disjoint set of\r\n",
      "movies, so no significant performance is obtained by memorizing\r\n",
      "movie-unique terms and their associated with observed labels.  In the\r\n",
      "labeled train/test sets, a negative review has a score <= 4 out of 10,\r\n",
      "and a positive review has a score >= 7 out of 10. Thus reviews with\r\n",
      "more neutral ratings are not included in the train/test sets. In the\r\n",
      "unsupervised set, reviews of any rating are included and there are an\r\n",
      "even number of reviews > 5 and <= 5.\r\n",
      "\r\n",
      "Files\r\n",
      "\r\n",
      "There are two top-level directories [train/, test/] corresponding to\r\n",
      "the training and test sets. Each contains [pos/, neg/] directories for\r\n",
      "the reviews with binary labels positive and negative. Within these\r\n",
      "directories, reviews are stored in text files named following the\r\n",
      "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\r\n",
      "the star rating for that review on a 1-10 scale. For example, the file\r\n",
      "[test/pos/200_8.txt] is the text for a positive-labeled test set\r\n",
      "example with unique id 200 and star rating 8/10 from IMDb. The\r\n",
      "[train/unsup/] directory has 0 for all ratings because the ratings are\r\n",
      "omitted for this portion of the dataset.\r\n",
      "\r\n",
      "We also include the IMDb URLs for each review in a separate\r\n",
      "[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will\r\n",
      "have its URL on line 200 of this file. Due the ever-changing IMDb, we\r\n",
      "are unable to link directly to the review, but only to the movie's\r\n",
      "review page.\r\n",
      "\r\n",
      "In addition to the review text files, we include already-tokenized bag\r\n",
      "of words (BoW) features that were used in our experiments. These \r\n",
      "are stored in .feat files in the train/test directories. Each .feat\r\n",
      "file is in LIBSVM format, an ascii sparse-vector format for labeled\r\n",
      "data.  The feature indices in these files start from 0, and the text\r\n",
      "tokens corresponding to a feature index is found in [imdb.vocab]. So a\r\n",
      "line with 0:7 in a .feat file means the first word in [imdb.vocab]\r\n",
      "(the) appears 7 times in that review.\r\n",
      "\r\n",
      "LIBSVM page for details on .feat file format:\r\n",
      "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\r\n",
      "\r\n",
      "We also include [imdbEr.txt] which contains the expected rating for\r\n",
      "each token in [imdb.vocab] as computed by (Potts, 2011). The expected\r\n",
      "rating is a good way to get a sense for the average polarity of a word\r\n",
      "in the dataset.\r\n",
      "\r\n",
      "Citing the dataset\r\n",
      "\r\n",
      "When using this dataset please cite our ACL 2011 paper which\r\n",
      "introduces it. This paper also contains classification results which\r\n",
      "you may want to compare against.\r\n",
      "\r\n",
      "\r\n",
      "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\r\n",
      "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\r\n",
      "  title     = {Learning Word Vectors for Sentiment Analysis},\r\n",
      "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\r\n",
      "  month     = {June},\r\n",
      "  year      = {2011},\r\n",
      "  address   = {Portland, Oregon, USA},\r\n",
      "  publisher = {Association for Computational Linguistics},\r\n",
      "  pages     = {142--150},\r\n",
      "  url       = {http://www.aclweb.org/anthology/P11-1015}\r\n",
      "}\r\n",
      "\r\n",
      "References\r\n",
      "\r\n",
      "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\r\n",
      "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\r\n",
      "636-659.\r\n",
      "\r\n",
      "Contact\r\n",
      "\r\n",
      "For questions/comments/corrections please contact Andrew Maas\r\n",
      "amaas@cs.stanford.edu\r\n"
     ]
    }
   ],
   "source": [
    "# IMDBデータセットの説明を表示\n",
    "!cat aclImdb/README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "概要\n",
    "\n",
    "このデータセットには、映画のレビューとそれに関連するバイナリが含まれています。\n",
    "センチメントの極性ラベル。のベンチマークとして機能することを目的としています。\n",
    "センチメント分類。この文書では、データセットがどのようにして\n",
    "集められたファイルと、提供されたファイルの使用方法を説明します。\n",
    "\n",
    "データセット \n",
    "\n",
    "コアデータセットには50,000件のレビューが含まれており、25,000件の列車に均等に分割されています。\n",
    "と25kのテストセットがあります。ラベルの全体的な分布はバランスが取れています（25k\n",
    "posと25k neg)。さらに、50,000個のラベルなしの\n",
    "文書を教師なし学習のために使用します．\n",
    "\n",
    "コレクション全体では，いずれかの\n",
    "なぜなら、同じ映画のレビューは相関性があるからです。\n",
    "評価を行います。さらに、訓練セットとテストセットには、不連続な\n",
    "ムービーを記憶しても大きなパフォーマンスは得られません。\n",
    "ムービーユニーク用語と観測されたラベルに関連付けられている。 の中では\n",
    "ラベル付けされた列車/テストセットでは、ネガティブなレビューは10点満点中4点未満となります。\n",
    "と肯定的なレビューは10点満点中7点以上のスコアを持っています。このように\n",
    "より多くの中立的な評価は、列車/テストセットには含まれていません。の中では\n",
    "監視されていないセット、任意の評価のレビューが含まれており、そこには\n",
    "偶数のレビュー > 5 と <= 5。\n",
    "\n",
    "ファイル\n",
    "\n",
    "に対応する 2 つのトップレベルディレクトリ [train/, test/] があります。\n",
    "学習セットとテストセットがあります．それぞれには，[pos/, neg/] ディレクトリが含まれています．\n",
    "レビューをポジティブとネガティブの二値ラベルで表示します。これらの中で\n",
    "ディレクトリに保存されます。\n",
    "規約[[id]_[レーティング].txt]で、[id]は一意のID、[レーティング]は\n",
    "は、そのレビューの星の評価を 1-10 のスケールで表示します。例えば、ファイルの\n",
    "test/pos/200_8.txt]は、陽性ラベルを付けたテストセットのテキストです。\n",
    "IMDbからユニークなID 200と星の評価8/10の例。固有ID200の\n",
    "train/unsup/] ディレクトリでは、レーティングが\n",
    "データセットのこの部分では省略されています。\n",
    "\n",
    "また、各レビューのIMDbのURLを別の\n",
    "urls_[pos, neg, unsup].txt]ファイルです。一意なID 200のレビューは\n",
    "のURLはこのファイルの200行目にあります。刻々と変化する IMDb のため、私たちは\n",
    "はレビューに直接リンクすることができず、映画の\n",
    "のレビューページを参照してください。\n",
    "\n",
    "レビューテキストファイルに加えて、既にトークン化されたバッグ\n",
    "of words (BoW)特徴量を用いて実験を行った。これらの \n",
    "は、train/test ディレクトリの .feat ファイルに格納されています。それぞれの.feat\n",
    "ファイルは LIBSVM フォーマットで、ラベル付き\n",
    "データを使用しています。 これらのファイルの特徴インデックスは 0 から始まり、テキスト\n",
    "特徴インデックスに対応するトークンは [imdb.vocab] にあります。ということは\n",
    ".featファイルの0:7の行は、[imdb.vocab]の最初の単語を意味します。\n",
    "(the)はそのレビューで7回登場しています。\n",
    "\n",
    ".featファイル形式の詳細についてはLIBSVMのページを参照してください。\n",
    "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n",
    "\n",
    "の予想視聴率を収録した[imdbEr.txt]も収録しています。\n",
    "(Potts, 2011)によって計算されたように、[imdb.vocab]の各トークンの 期待される\n",
    "レーティングは言葉の平均的な極性を知るための良い方法です。\n",
    "をデータセットの中で使用しています。\n",
    "\n",
    "データセットを引用する\n",
    "\n",
    "このデータセットを使用する際には、我々のACL 2011の論文を引用してください。\n",
    "を紹介しています。また，本論文では，以下のような分類結果も掲載しています．\n",
    "と比較してみてはいかがでしょうか。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "train_review = load_files('./aclImdb/train/', encoding='utf-8')\n",
    "x_train, y_train = train_review.data, train_review.target\n",
    "\n",
    "test_review = load_files('./aclImdb/test/', encoding='utf-8')\n",
    "x_test, y_test = test_review.data, test_review.target\n",
    "\n",
    "# ラベルの0,1と意味の対応の表示\n",
    "print(train_review.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.<br /><br />It is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. In terms of explaining the motives and actions of the two young suicide/murderers it is better than 'Elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see. <br /><br />Flawed but honest with a terrible honesty.\n"
     ]
    }
   ],
   "source": [
    "print(\"x : {}\".format(x_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】BoWのスクラッチ実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下の3文のBoWを求められるプログラムをscikit-learnを使わずに作成してください。1-gramと2-gramで計算してください。\n",
    "\n",
    "\n",
    "- This movie is SOOOO funny!!!\n",
    "- What a movie! I never\n",
    "- best movie ever!!!!! this movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_dataset =  [\"This movie is SOOOO funny!!!\", \"What a movie! I never\",  \"best movie ever!!!!! this movie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'movie', 'is', 'SOOOO', 'funny!!!'], ['What', 'a', 'movie!', 'I', 'never'], ['best', 'movie', 'ever!!!!!', 'this', 'movie']]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_dataset = []\n",
    "for data in mini_dataset:\n",
    "    preprocessed_dataset.append(data.split(' '))\n",
    "print(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __1-gram__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Counter({'This': 1, 'movie': 1, 'is': 1, 'SOOOO': 1, 'funny!!!': 1}),\n",
      " Counter({'What': 1, 'a': 1, 'movie!': 1, 'I': 1, 'never': 1}),\n",
      " Counter({'movie': 2, 'best': 1, 'ever!!!!!': 1, 'this': 1})]\n"
     ]
    }
   ],
   "source": [
    "frequency_list = []\n",
    "for data in preprocessed_dataset:\n",
    "    frequency_list.append(Counter(data))\n",
    "    \n",
    "pprint.pprint(frequency_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'This movie is SOOOO funny!!!': {'SOOOO': 1,\n",
      "                                  'This': 1,\n",
      "                                  'funny!!!': 1,\n",
      "                                  'is': 1,\n",
      "                                  'movie': 1},\n",
      " 'What a movie! I never': {'I': 1, 'What': 1, 'a': 1, 'movie!': 1, 'never': 1},\n",
      " 'best movie ever!!!!! this movie': {'best': 1,\n",
      "                                     'ever!!!!!': 1,\n",
      "                                     'movie': 2,\n",
      "                                     'this': 1}}\n"
     ]
    }
   ],
   "source": [
    "to_dataframe = dict()\n",
    "for index, value in zip(mini_dataset, frequency_list):\n",
    "    to_dataframe[index] = dict(value)\n",
    "    \n",
    "pprint.pprint(to_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>This</th>\n",
       "      <th>movie</th>\n",
       "      <th>is</th>\n",
       "      <th>SOOOO</th>\n",
       "      <th>funny!!!</th>\n",
       "      <th>What</th>\n",
       "      <th>a</th>\n",
       "      <th>movie!</th>\n",
       "      <th>I</th>\n",
       "      <th>never</th>\n",
       "      <th>best</th>\n",
       "      <th>ever!!!!!</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>This movie is SOOOO funny!!!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>What a movie! I never</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>best movie ever!!!!! this movie</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 This  movie  is  SOOOO  funny!!!  What  a  \\\n",
       "This movie is SOOOO funny!!!        1      1   1      1         1     0  0   \n",
       "What a movie! I never               0      0   0      0         0     1  1   \n",
       "best movie ever!!!!! this movie     0      2   0      0         0     0  0   \n",
       "\n",
       "                                 movie!  I  never  best  ever!!!!!  this  \n",
       "This movie is SOOOO funny!!!          0  0      0     0          0     0  \n",
       "What a movie! I never                 1  1      1     0          0     0  \n",
       "best movie ever!!!!! this movie       0  0      0     1          1     1  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1gram = pd.DataFrame.from_dict(to_dataframe, orient='index').sort_index()\n",
    "df_1gram = df_1gram.fillna(0).astype('int')\n",
    "\n",
    "df_1gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __2-gram__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_frequency_counter = list()\n",
    "for data in preprocessed_dataset:\n",
    "    temp_to_list = list()\n",
    "    for index in range(len(data) -2 +1):\n",
    "        temp_data = data[index : index + 2]\n",
    "        temp_two_words = temp_data[0] + \" \" + temp_data[1]\n",
    "        temp_to_list.append(temp_two_words)\n",
    "    \n",
    "    to_frequency_counter.append(temp_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This movie', 'movie is', 'is SOOOO', 'SOOOO funny!!!'],\n",
       " ['What a', 'a movie!', 'movie! I', 'I never'],\n",
       " ['best movie', 'movie ever!!!!!', 'ever!!!!! this', 'this movie']]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_frequency_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Counter({'This movie': 1, 'movie is': 1, 'is SOOOO': 1, 'SOOOO funny!!!': 1}),\n",
      " Counter({'What a': 1, 'a movie!': 1, 'movie! I': 1, 'I never': 1}),\n",
      " Counter({'best movie': 1,\n",
      "          'movie ever!!!!!': 1,\n",
      "          'ever!!!!! this': 1,\n",
      "          'this movie': 1})]\n"
     ]
    }
   ],
   "source": [
    "frequency_list = []\n",
    "for data in to_frequency_counter:\n",
    "    frequency_list.append(Counter(data))\n",
    "    \n",
    "pprint.pprint(frequency_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'This movie is SOOOO funny!!!': {'SOOOO funny!!!': 1,\n",
      "                                  'This movie': 1,\n",
      "                                  'is SOOOO': 1,\n",
      "                                  'movie is': 1},\n",
      " 'What a movie! I never': {'I never': 1,\n",
      "                           'What a': 1,\n",
      "                           'a movie!': 1,\n",
      "                           'movie! I': 1},\n",
      " 'best movie ever!!!!! this movie': {'best movie': 1,\n",
      "                                     'ever!!!!! this': 1,\n",
      "                                     'movie ever!!!!!': 1,\n",
      "                                     'this movie': 1}}\n"
     ]
    }
   ],
   "source": [
    "to_dataframe = dict()\n",
    "for index, value in zip(mini_dataset, frequency_list):\n",
    "    to_dataframe[index] = dict(value)\n",
    "    \n",
    "pprint.pprint(to_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>This movie</th>\n",
       "      <th>movie is</th>\n",
       "      <th>is SOOOO</th>\n",
       "      <th>SOOOO funny!!!</th>\n",
       "      <th>What a</th>\n",
       "      <th>a movie!</th>\n",
       "      <th>movie! I</th>\n",
       "      <th>I never</th>\n",
       "      <th>best movie</th>\n",
       "      <th>movie ever!!!!!</th>\n",
       "      <th>ever!!!!! this</th>\n",
       "      <th>this movie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>This movie is SOOOO funny!!!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>What a movie! I never</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>best movie ever!!!!! this movie</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 This movie  movie is  is SOOOO  \\\n",
       "This movie is SOOOO funny!!!              1         1         1   \n",
       "What a movie! I never                     0         0         0   \n",
       "best movie ever!!!!! this movie           0         0         0   \n",
       "\n",
       "                                 SOOOO funny!!!  What a  a movie!  movie! I  \\\n",
       "This movie is SOOOO funny!!!                  1       0         0         0   \n",
       "What a movie! I never                         0       1         1         1   \n",
       "best movie ever!!!!! this movie               0       0         0         0   \n",
       "\n",
       "                                 I never  best movie  movie ever!!!!!  \\\n",
       "This movie is SOOOO funny!!!           0           0                0   \n",
       "What a movie! I never                  1           0                0   \n",
       "best movie ever!!!!! this movie        0           1                1   \n",
       "\n",
       "                                 ever!!!!! this  this movie  \n",
       "This movie is SOOOO funny!!!                  0           0  \n",
       "What a movie! I never                         0           0  \n",
       "best movie ever!!!!! this movie               1           1  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2gram = pd.DataFrame.from_dict(to_dataframe, orient='index').sort_index()\n",
    "df_2gram = df_2gram.fillna(0).astype('int')\n",
    "\n",
    "df_2gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】TF-IDFの計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB映画レビューデータセットをTF-IDFによりベクトル化してください。NLTKのストップワードを利用し、最大の語彙数は5000程度に設定してください。テキストクリーニングやステミングなどの前処理はこの問題では要求しません。\n",
    "\n",
    "\n",
    "TF-IDFの計算にはscikit-learnの以下のどちらかのクラスを使用してください。\n",
    "\n",
    "\n",
    "[sklearn.feature_extraction.text.TfidfVectorizer — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "\n",
    "[sklearn.feature_extraction.text.TfidfTransformer — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop word : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/takahiromotoki/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "stop_words = nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(\"stop word : {}\".format(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1-gram, 最大語彙数:5000, 全ストップワード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=5000)\n",
    "\n",
    "x_train_1gram = vectorizer.fit_transform(x_train)\n",
    "x_test_1gram = vectorizer.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '10', '100', '11', '12', '13', '13th', '14', '15', '16', '17', '18', '1930', '1940', '1940s', '1950', '1950s', '1960', '1960s', '1968', '1970', '1970s', '1971', '1972', '1979', '1980', '1980s', '1981', '1982', '1990', '1995', '1997', '1998', '1999', '19th', '1st', '20', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '20th', '23', '24', '25', '2nd', '30', '3000', '35', '3rd', '40', '45', '50', '50s', '60', '60s', '70', '70s', '80', '80s', '90', '90s', '95', '99', 'aaron', 'abandoned', 'abc', 'abilities', 'ability', 'able', 'aboard', 'absence', 'absent', 'absolute', 'absolutely', 'absurd', 'abuse', 'abused', 'abysmal', 'academy', 'accent', 'accents', 'accept', 'acceptable', 'accepted', 'accident', 'accidentally', 'accompanied', 'accomplished', 'according', 'account', 'accurate', 'accurately', 'accused', 'achieve']\n",
      "\n",
      "\n",
      "(25000, 5000) (25000, 5000)\n",
      "\n",
      "\n",
      "  (0, 267)\t0.08381220735492088\n",
      "  (0, 201)\t0.08282260831175318\n",
      "  (0, 475)\t0.13734353234274008\n",
      "  (0, 111)\t0.14007222716965326\n",
      "  (0, 114)\t0.07492419060647448\n",
      "  (0, 496)\t0.12821983936481943\n",
      "  (0, 478)\t0.11297910404114764\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names()[:100])\n",
    "print(\"\\n\")\n",
    "print(x_train_1gram.shape, x_test_1gram.shape)\n",
    "print(\"\\n\")\n",
    "print(x_train_1gram[0, 0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2-gram, 最大語彙数:5000, 全ストップワード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=5000, ngram_range = (2, 2))\n",
    "\n",
    "x_train_2gram = vectorizer.fit_transform(x_train)\n",
    "x_test_2gram = vectorizer.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000) (25000, 5000)\n",
      "\n",
      "\n",
      "  (0, 418)\t0.24948479772013363\n"
     ]
    }
   ],
   "source": [
    "print(x_train_2gram.shape, x_test_2gram.shape)\n",
    "print(\"\\n\")\n",
    "print(x_train_2gram[0, 0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1-gram, 最大語彙数:1000, 全ストップワード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=1000)\n",
    "\n",
    "x_train_1gram_features = vectorizer.fit_transform(x_train)\n",
    "x_test_1gram_features = vectorizer.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 1000) (25000, 1000)\n",
      "\n",
      "\n",
      "  (0, 43)\t0.14077780520698466\n",
      "  (0, 26)\t0.13911559410756655\n",
      "  (0, 302)\t0.2816563515587199\n",
      "  (0, 362)\t0.13979900996516081\n",
      "  (0, 319)\t0.13597214591742793\n",
      "  (0, 85)\t0.2306933757359183\n",
      "  (0, 14)\t0.12584876885323995\n",
      "  (0, 368)\t0.16004244500207904\n",
      "  (0, 101)\t0.26326490402478975\n",
      "  (0, 161)\t0.2125088870952694\n",
      "  (0, 202)\t0.1984760153439908\n",
      "  (0, 86)\t0.18976889886471296\n",
      "  (0, 100)\t0.2123307784669702\n",
      "  (0, 269)\t0.08963869263647614\n",
      "  (0, 483)\t0.19689658338015378\n",
      "  (0, 195)\t0.1463229475450414\n"
     ]
    }
   ],
   "source": [
    "print(x_train_1gram_features.shape, x_test_1gram_features.shape)\n",
    "print(\"\\n\")\n",
    "print(x_train_1gram_features[0, 0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1-gram, 最大語彙数:5000, ストップワード:無し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=None, max_features=5000)\n",
    "\n",
    "x_train_1gram_stopwords = vectorizer.fit_transform(x_train)\n",
    "x_test_1gram_stopwords = vectorizer.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000) (25000, 5000)\n",
      "\n",
      "\n",
      "  (0, 293)\t0.03679449748819134\n",
      "  (0, 268)\t0.07605535650029367\n",
      "  (0, 200)\t0.07515734521535346\n",
      "  (0, 483)\t0.12463233751978506\n",
      "  (0, 107)\t0.12710849062911136\n",
      "  (0, 110)\t0.06798992899614642\n",
      "  (0, 233)\t0.0479569933322464\n",
      "  (0, 458)\t0.12070301324957379\n",
      "  (0, 487)\t0.10252284608787063\n",
      "  (0, 428)\t0.03640421567456567\n"
     ]
    }
   ],
   "source": [
    "print(x_train_1gram_stopwords.shape, x_test_1gram_stopwords.shape)\n",
    "print(\"\\n\")\n",
    "print(x_train_1gram_stopwords[0, 0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】TF-IDFを用いた学習 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問題2で求めたベクトルを用いてIMDB映画レビューデータセットの学習・推定を行なってください。\n",
    "\n",
    "モデルは2値分類が行える任意のものを利用してください。\n",
    "\n",
    "\n",
    "ここでは精度の高さは求めませんが、最大の語彙数やストップワード、n-gramの数を変化させて影響を検証してみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1-gram, 最大語彙数:5000, 全ストップワード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr_1gram = LogisticRegression(solver=\"lbfgs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lr_1gram.fit(x_train_1gram, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1gram = clf_lr_1gram.predict(x_test_1gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.56      0.56     12500\n",
      "           1       0.56      0.55      0.56     12500\n",
      "\n",
      "    accuracy                           0.56     25000\n",
      "   macro avg       0.56      0.56      0.56     25000\n",
      "weighted avg       0.56      0.56      0.56     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_1gram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2-gram, 最大語彙数:5000, 全ストップワード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr_2gram = LogisticRegression(solver=\"lbfgs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lr_2gram.fit(x_train_2gram, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_2gram = clf_lr_2gram.predict(x_test_2gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.57      0.57     12500\n",
      "           1       0.57      0.56      0.56     12500\n",
      "\n",
      "    accuracy                           0.56     25000\n",
      "   macro avg       0.56      0.56      0.56     25000\n",
      "weighted avg       0.56      0.56      0.56     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_2gram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1-gram, 最大語彙数:1000, 全ストップワード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr_1gram_features = LogisticRegression(solver=\"lbfgs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lr_1gram_features.fit(x_train_1gram_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1gram_features = clf_lr_1gram_features.predict(x_test_1gram_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.62      0.57     12500\n",
      "           1       0.54      0.46      0.50     12500\n",
      "\n",
      "    accuracy                           0.54     25000\n",
      "   macro avg       0.54      0.54      0.53     25000\n",
      "weighted avg       0.54      0.54      0.53     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_1gram_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1-gram, 最大語彙数:5000, ストップワード:無し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr_1gram_stopwords = LogisticRegression(solver=\"lbfgs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lr_1gram_stopwords.fit(x_train_1gram_stopwords, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1gram_stopwords = clf_lr_1gram_stopwords.predict(x_test_1gram_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.65     12500\n",
      "           1       0.65      0.48      0.55     12500\n",
      "\n",
      "    accuracy                           0.61     25000\n",
      "   macro avg       0.62      0.61      0.60     25000\n",
      "weighted avg       0.62      0.61      0.60     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_1gram_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題4】TF-IDFのスクラッチ実装 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下の3文のTF-IDFを求められるプログラムをscikit-learnを使わずに作成してください。\n",
    "\n",
    "標準的な式と、scikit-learnの採用している式の2種類を作成してください。正規化は不要です。\n",
    "\n",
    "\n",
    "- This movie is SOOOO funny!!!\n",
    "- What a movie! I never\n",
    "- best movie ever!!!!! this movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考kaggleノートブック：[Creating TF-IDF Model from Scratch](https://www.kaggle.com/yassinehamdaoui1/creating-tf-idf-model-from-scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 標準的なTF-IDFの式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie', 'is', 'funny', 'what', 'a', 'soooo', 'i', 'ever', 'best', 'never', 'this'}\n"
     ]
    }
   ],
   "source": [
    "first_sentence = \"This movie is SOOOO funny!!!\"\n",
    "second_sentence = \"What a movie! I never\"\n",
    "third_sentence = \"best movie ever!!!!! this movie\"\n",
    "\n",
    "#split so each word have their own string\n",
    "first_sentence = first_sentence.lower().replace(\"!\", \"\").strip().split(\" \")\n",
    "second_sentence = second_sentence.lower().replace(\"!\", \"\").strip().split(\" \")\n",
    "third_sentence = third_sentence.lower().replace(\"!\", \"\").strip().split(\" \")\n",
    "\n",
    "#join them to remove common duplicate words\n",
    "total= set(first_sentence).union(set(second_sentence)).union(set(third_sentence))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDictA = dict.fromkeys(total, 0) \n",
    "wordDictB = dict.fromkeys(total, 0)\n",
    "wordDictC = dict.fromkeys(total, 0)\n",
    "\n",
    "for word in first_sentence:\n",
    "    wordDictA[word]+=1\n",
    "    \n",
    "for word in second_sentence:\n",
    "    wordDictB[word]+=1\n",
    "    \n",
    "for word in third_sentence:\n",
    "    wordDictC[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>is</th>\n",
       "      <th>funny</th>\n",
       "      <th>what</th>\n",
       "      <th>a</th>\n",
       "      <th>soooo</th>\n",
       "      <th>i</th>\n",
       "      <th>ever</th>\n",
       "      <th>best</th>\n",
       "      <th>never</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie  is  funny  what  a  soooo  i  ever  best  never  this\n",
       "0      1   1      1     0  0      1  0     0     0      0     1\n",
       "1      1   0      0     1  1      0  1     0     0      1     0\n",
       "2      2   0      0     0  0      0  0     1     1      0     1"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_df = pd.DataFrame([wordDictA, wordDictB, wordDictC])\n",
    "frequency_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>is</th>\n",
       "      <th>funny</th>\n",
       "      <th>what</th>\n",
       "      <th>a</th>\n",
       "      <th>soooo</th>\n",
       "      <th>i</th>\n",
       "      <th>ever</th>\n",
       "      <th>best</th>\n",
       "      <th>never</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie   is  funny  what    a  soooo    i  ever  best  never  this\n",
       "0    0.2  0.2    0.2   0.0  0.0    0.2  0.0   0.0   0.0    0.0   0.2\n",
       "1    0.2  0.0    0.0   0.2  0.2    0.0  0.2   0.0   0.0    0.2   0.0\n",
       "2    0.4  0.0    0.0   0.0  0.0    0.0  0.0   0.2   0.2    0.0   0.2"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeTF(wordDict, doc):\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(corpusCount)\n",
    "    return(tfDict)\n",
    "#running our sentences through the tf function:\n",
    "tfFirst = computeTF(wordDictA, first_sentence)\n",
    "tfSecond = computeTF(wordDictB, second_sentence)\n",
    "tfThird = computeTF(wordDictC, third_sentence)\n",
    "\n",
    "#Converting to dataframe for visualization\n",
    "tf = pd.DataFrame([tfFirst, tfSecond, tfThird])\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie': -0.2876820724517809,\n",
       " 'is': 1.0986122886681098,\n",
       " 'funny': 1.0986122886681098,\n",
       " 'what': 1.0986122886681098,\n",
       " 'a': 1.0986122886681098,\n",
       " 'soooo': 1.0986122886681098,\n",
       " 'i': 1.0986122886681098,\n",
       " 'ever': 1.0986122886681098,\n",
       " 'best': 1.0986122886681098,\n",
       " 'never': 1.0986122886681098,\n",
       " 'this': 0.4054651081081644}"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "\n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = np.log(N / (frequency_df[word].sum()))\n",
    "        \n",
    "    return(idfDict)\n",
    "\n",
    "#inputing our sentences in the log file\n",
    "idfs = computeIDF([wordDictA, wordDictB, wordDictC])\n",
    "idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>is</th>\n",
       "      <th>funny</th>\n",
       "      <th>what</th>\n",
       "      <th>a</th>\n",
       "      <th>soooo</th>\n",
       "      <th>i</th>\n",
       "      <th>ever</th>\n",
       "      <th>best</th>\n",
       "      <th>never</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.057536</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.057536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.115073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      movie        is     funny      what         a     soooo         i  \\\n",
       "0 -0.057536  0.219722  0.219722  0.000000  0.000000  0.219722  0.000000   \n",
       "1 -0.057536  0.000000  0.000000  0.219722  0.219722  0.000000  0.219722   \n",
       "2 -0.115073  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       ever      best     never      this  \n",
       "0  0.000000  0.000000  0.000000  0.081093  \n",
       "1  0.000000  0.000000  0.219722  0.000000  \n",
       "2  0.219722  0.219722  0.000000  0.081093  "
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return(tfidf)\n",
    "\n",
    "#running our two sentences through the IDF:\n",
    "idfFirst = computeTFIDF(tfFirst, idfs)\n",
    "idfSecond = computeTFIDF(tfSecond, idfs)\n",
    "idfThird = computeTFIDF(tfThird, idfs)\n",
    "\n",
    "#putting it in a dataframe\n",
    "tf_idf = pd.DataFrame([idfFirst, idfSecond, idfThird])\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- scikit-learnの式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie': 0.7768564486857903,\n",
       " 'is': 1.6931471805599454,\n",
       " 'funny': 1.6931471805599454,\n",
       " 'what': 1.6931471805599454,\n",
       " 'a': 1.6931471805599454,\n",
       " 'soooo': 1.6931471805599454,\n",
       " 'i': 1.6931471805599454,\n",
       " 'ever': 1.6931471805599454,\n",
       " 'best': 1.6931471805599454,\n",
       " 'never': 1.6931471805599454,\n",
       " 'this': 1.2876820724517808}"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "\n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = np.log((1 + N) / (frequency_df[word].sum() + 1)) + 1\n",
    "        \n",
    "    return(idfDict)\n",
    "#inputing our sentences in the log file\n",
    "idfs = computeIDF([wordDictA, wordDictB, wordDictC])\n",
    "idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>is</th>\n",
       "      <th>funny</th>\n",
       "      <th>what</th>\n",
       "      <th>a</th>\n",
       "      <th>soooo</th>\n",
       "      <th>i</th>\n",
       "      <th>ever</th>\n",
       "      <th>best</th>\n",
       "      <th>never</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.776856</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.776856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.553713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.287682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      movie        is     funny      what         a     soooo         i  \\\n",
       "0  0.776856  1.693147  1.693147  0.000000  0.000000  1.693147  0.000000   \n",
       "1  0.776856  0.000000  0.000000  1.693147  1.693147  0.000000  1.693147   \n",
       "2  1.553713  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       ever      best     never      this  \n",
       "0  0.000000  0.000000  0.000000  1.287682  \n",
       "1  0.000000  0.000000  1.693147  0.000000  \n",
       "2  1.693147  1.693147  0.000000  1.287682  "
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return(tfidf)\n",
    "\n",
    "#running our two sentences through the IDF:\n",
    "idfFirst = computeTFIDF(wordDictA, idfs)\n",
    "idfSecond = computeTFIDF(wordDictB, idfs)\n",
    "idfThird = computeTFIDF(wordDictC, idfs)\n",
    "#putting it in a dataframe\n",
    "tf_idf = pd.DataFrame([idfFirst, idfSecond, idfThird])\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "語彙の一覧 : dict_keys(['this', 'movie', 'is', 'very', 'good', 'film', 'a', 'bad'])\n",
      "thisのベクトル : \n",
      "[ 0.04490024 -0.02213435 -0.0218689   0.01280147  0.01727903 -0.04213432\n",
      "  0.04743863 -0.03321018  0.02952689  0.04063499]\n",
      "movieのベクトル : \n",
      "[-0.03478461 -0.03077289 -0.02795689 -0.00443146 -0.00632171  0.00614787\n",
      "  0.04722304  0.04706873  0.02717129  0.03652719]\n",
      "isのベクトル : \n",
      "[-0.00414363 -0.00782995 -0.02804689 -0.00589878  0.03851486 -0.0365783\n",
      " -0.04637689 -0.02485681 -0.00470399 -0.03171895]\n",
      "veryのベクトル : \n",
      "[-0.0183737  -0.04111299  0.01003001  0.00445909 -0.0094394   0.0407518\n",
      "  0.01166302  0.00751845  0.03299264  0.00412863]\n",
      "goodのベクトル : \n",
      "[ 0.0290839  -0.0222785   0.03128101  0.00465455  0.00985504 -0.0314997\n",
      "  0.04443805 -0.03294802  0.02218802 -0.02682054]\n",
      "filmのベクトル : \n",
      "[ 0.02322064 -0.01222273  0.03773014 -0.03526619  0.03218704 -0.04008739\n",
      "  0.01164205  0.01468769 -0.02396114  0.01799934]\n",
      "aのベクトル : \n",
      "[ 0.01941237  0.0187009  -0.03106834  0.04038012  0.03348972  0.00066411\n",
      " -0.04563151  0.04942786 -0.03038042 -0.02839704]\n",
      "badのベクトル : \n",
      "[-0.04148511 -0.04318393 -0.04954907  0.04679543 -0.01559247  0.0146984\n",
      " -0.01112665  0.014167   -0.01166481 -0.00372365]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takahiromotoki/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [['this', 'movie', 'is', 'very', 'good'], ['this', 'film', 'is', 'a', 'good'], ['very', 'bad', 'very', 'very', 'bad']]\n",
    "model = Word2Vec(min_count=1, size=10) # 次元数を10に設定\n",
    "model.build_vocab(sentences) # 準備\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter) # 学習\n",
    "print(\"語彙の一覧 : {}\".format(model.wv.vocab.keys()))\n",
    "for vocab in model.wv.vocab.keys():\n",
    "  print(\"{}のベクトル : \\n{}\".format(vocab, model.wv[vocab]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 0.588284432888031),\n",
       " ('film', 0.34295129776000977),\n",
       " ('very', 0.036548078060150146)]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=\"good\", topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takahiromotoki/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASgAAAEhCAYAAADMCD3RAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy86wFpkAAAACXBIWXMAAAsTAAALEwEAmpwYAAASuElEQVR4nO3dfXBV9Z3H8c8hRLyQwRsgVRLkaUUISQzRG0WByOIM0bFKECXToaKJSCnqOmozG6Z2Fe0iFjo4MFAmWuMDYbqINE6xLG5rHYiMlhuSkMiDCA3LXlyNyoWmCTQPZ/9gyDYBSULuwzfx/fqL+8u9l+/5g/ecX+49B8d1XQGARf2iPQAAfBsCBcAsAgXALAIFwCwCBcAsAgXArP7defKwYcPc0aNHh2kUAN9V5eXlX7mum9BxvVuBGj16tPx+f+imAgBJjuMcvdA6WzwAZhEoAGYRKABmEShjgsGg1q1bJ0n64IMP9P3vf/+Cz1uwYIH27dsXydGAiCNQxvxjoC7mlVde0cSJEyMwERA9BMqYwsJCHT58WJMmTVJBQYHq6+t17733asKECZo3b57O3X1i+vTp8vv9amlp0YMPPqjU1FSlpaVp1apVUT4CIHS69TUDhN/y5ctVU1OjyspKffDBB5o1a5Y++eQTJSYmasqUKfrwww81derUtudXVlYqEAiopqZG0tkzMKCvIFAGlFYEtGL7QR0PNmqIe1KnTje3/ezGG2/UiBEjJEmTJk1SbW1tu0CNHTtWR44c0WOPPaY777xTM2fOjPj8QLiwxYuy0oqAlmypViDYKFfSF6dO64tTp1VaEZAkDRgwoO25MTExam5ubvf6+Ph4VVVVafr06Vq/fr0WLFgQyfGBsCJQUbZi+0E1NrW0PXYu86jlTINWbD/Ypdd/9dVXam1t1Zw5c/Tzn/9ce/bsCdeoiKL169frjTfeiPYYEccWL8qOBxvbPY7xDNaApIna/cs8FYz+nq688sqLvj4QCCgvL0+tra2SpBdeeCFssyJ6Fi1aFO0RosLpzj3JfT6fy7V4oTVl+fsKdIiUJCV5PfqwcEYUJkJP1dbW6vbbb9fkyZO1a9cuZWZmKi8vT88884y+/PJLlZSU6JprrlF+fr6OHDmigQMHqqioSKmpqRo7dqwqKyvl9XolSePGjVNZWZl+9atfKS4uTj/5yU90+PBhPfLII6qrq9PAgQP18ssva8KECdE96B5yHKfcdV1fx3W2eFFWkD1entiYdmue2BgVZI+P0kQIhc8++0xPPfWUDhw4oAMHDmjjxo0qKyvTypUrtWzZMj3zzDPKyMjQ3r17tWzZMs2fP1/9+vXTrFmz9Nvf/laS9PHHH2vUqFHnnUUvXLhQa9asUXl5uVauXKnFixdH4xAjgi1elOVkJElS26d4iV6PCrLHt62jdxozZozS0tIkSSkpKbrtttvkOI7S0tJUW1uro0eP6u2335YkzZgxQ19//bVOnTql3NxcPffcc8rLy9NvfvMb5ebmtnvf+vp67dq1S/fdd1/b2pkzZyJ3YBFGoAzIyUgiSL1cx6+KnHH//6y4X79+bZ/G9uvXT83NzYqNjb3g+9x888367LPPVFdXp9LSUj399NPtft7a2iqv16vKysqwHYslbPGAHursqyIXMm3aNJWUlEg6e83lsGHDNHjwYDmOo9mzZ+vJJ59UcnKyhg4d2u51gwcP1pgxY/TWW29JklzXVVVVVdiOLdoIFNBDHb8qIp0Nx8W+KvLss8+qvLxc1113nQoLC/X666+3/Sw3N1cbNmw4b3t3TklJiX79618rPT1dKSkpeuedd0JzIAbxKR7QQ2MK39WF/hU5kv6y/M5Ij9Mr8SkeECaJXk+31tF1nQbKcZyFjuP4Hcfx19XVRWImoFfhqyLh02mgXNctcl3X57quLyHhvP90AfjOy8lI0gv3pCnJ65Gjs1+yfeGeND6ZDQG+ZgCEAF8VCQ9+BwXALAIFwCwCBcAsAgXALAIFwCwCBcAsAgXALAIFwCwCBcAsAgXALAIFwCwCBcAsAgXALAIFwCwCBcAsAgXALAIFwCwCBcAsAgXALAIFwCwCBcAsAgXALAIFwCwCBcAsAgXALAIFwCwCBcAsAgXALAIFwCwCBcAsAgXALAIFwCwCBcAsAgXALAIFwCwCBcAsAgXALAIFwCwCBcAsAgXALAIFwCwCBcAsAgXALAIFwCwCBcAsAgXALAIFwCwCBcAsAgXALAIFwKxOA+U4zkLHcfyO4/jr6uoiMRMASOpCoFzXLXJd1+e6ri8hISESMwGAJLZ4AAwjUADMIlAAzCJQAMwiUADMIlAAzCJQAMwiUADMIlAAzCJQAMwiUADMCnugamtrlZqaGvHXAuj9OIMCYFZEAtXc3Kx58+YpOTlZ9957rxoaGvTcc88pMzNTqampWrhwoVzXlSSVl5crPT1d6enpWrt2bSTGA2BURAJ18OBBLV68WPv379fgwYO1bt06Pfroo9q9e7dqamrU2NiorVu3SpLy8vK0Zs0aVVVVRWI0AIb1D8ebllYEtGL7QR0PNmqIe1LDrkrUlClTJEk//OEPtXr1ao0ZM0a/+MUv1NDQoG+++UYpKSmaNm2agsGgsrKyJEn333+/tm3bFo4RAfQCIT+DKq0IaMmWagWCjXIlfXHqtIINzSqtCLQ9x3EcLV68WJs3b1Z1dbUefvhhnT59OtSjAOjlQh6oFdsPqrGppd1a86kv9W9FWyRJGzdu1NSpUyVJw4YNU319vTZv3ixJ8nq98nq9KisrkySVlJSEejwAvUjIt3jHg43n/yVDRujIji1KTl6niRMn6sc//rFOnDih1NRUXXXVVcrMzGx7bnFxsfLz8+U4jmbOnBnq8QD0Is65T8+6wufzuX6//6LPmbL8fQUuEKkkr0cfFs7o9oAA+j7Hccpd1/V1XA/5Fq8ge7w8sTHt1jyxMSrIHh/qvwpAHxfyLV5ORpIktX2Kl+j1qCB7fNs6AHRVWL5mkJORRJAA9BiXugAwi0ABMItAATCLQAEwi0ABMItAATCLQAEIi5ycHN1www1KSUlRUVHRJb1HWL4HBQCvvvqqhgwZosbGRmVmZmrOnDkaOnRot96DQAEIiX+8D1yi16Or/7JV+z/6oyTp2LFjOnToEIECEHnn7gN37lZLh/d+rIqd21X8H+8o95ZrNH369Eu65xuBAtBjHe8D13qmQRowSKt3/LfShzTro48+uqT3JVAAeqzjfeA8Y27QXyu2afeKB1T40Q2aPHnyJb0vgQLQY4leT7v7wDn9Y3Xl3KVK8npU2oP7wPE1AwA9Fq77wHEGBaDHwnUfOAIFICTCcR84tngAzCJQAMwiUADMIlAAzCJQAMwiUADMIlAAzCJQAMwiUAi5W265JdojoI8gUAi5Xbt2RXsE9BEECiEXFxcnSfr888+VlZWlSZMmKTU1VTt37ozyZOhtOr0Wz3GchZIWStLIkSPDPhD6jo0bNyo7O1s//elP1dLSooaGhmiPhF6m00C5rlskqUiSfD6fG/aJ0GdkZmYqPz9fTU1NysnJ0aRJk6I9EnoZtngIidKKgKYsf19jCt9VY1OLSisCysrK0o4dO5SUlKQHH3xQb7zxRrTHRC/D7VbQYx1vmO+60pIt1fry+P/oodt9evjhh3XmzBnt2bNH8+fPj/K06E0IFHqs4w3zJamxqUUrXtui1f/6kGJjYxUXF8cZFLqNQKHHOt4wf+STmyVJzf+UpUNvvRiNkdBH8Dso9Fii19OtdaCrCBR6LFw3zAfY4qHHwnXDfIBAISTCccN8gC0eALMIFACzCBQAswgUALMIFACzCBQAswgUALMIFACzCBQAswgUALMIFACzCBQAswgUALMIFACzCBQAswgUALMIFACzCBQAswgUALMIFACzCBQAswgUALMIFACzCBQAswgUALMIFACzCBQAswgUALMIFACzCBQAswgUALMIFACzCBQAswgUALMIFACzCBQAswgUALMIFACzCBQAswgUALMIFACzCBQAszoNlOM4Cx3H8TuO46+rq4vETAAgqQuBcl23yHVdn+u6voSEhEjMBACS2OIBMIxAATCLQAEwi0ABMItAATCLQAEwi0ABMItAATCLQAEwi0ABMItAATCLQAEwi0ABMItAATCLQAEwi0ABMItAATCLQAEwq88HynVdtba2RnsMAJegf7QH6KrCwkJdffXVeuSRRyRJzz77rOLi4uS6rjZt2qQzZ85o9uzZWrp0qWpra5Wdna2bbrpJ5eXlmjt3rk6cOKGXXnpJkvTyyy9r3759WrVqVRSPCEBnes0ZVG5urjZt2tT2eNOmTUpISNChQ4f05z//WZWVlSovL9eOHTskSYcOHdLixYv1ySef6KmnntLvfvc7NTU1SZKKi4uVn58fleMA0HXmz6BKKwJasf2gjgcb9cX+Wr36XrluuLK/4uPjVV1drffee08ZGRmSpPr6eh06dEgjR47UqFGjNHnyZElSXFycZsyYoa1btyo5OVlNTU1KS0uL5mEB6ALTgSqtCGjJlmo1NrVIki4bd4uW/PIVTUvqr9zcXB09elRLlizRj370o3avq62t1aBBg9qtLViwQMuWLdOECROUl5cXsWMAcOlMB2rF9oNtcZKkgROm6Zv/XKN3y/+qNf++W9XV1frZz36mefPmKS4uToFAQLGxsRd8r5tuuknHjh3Tnj17tHfv3kgdAoAeMB2o48HGdo8vSxil1r83KmbQEA0fPlzDhw/X/v37dfPNN0s6u5XbsGGDYmJiLvh+c+fOVWVlpeLj48M+O4CeMx2oRK9HgQ6RSnxorZK8nrbHjz/+uB5//PHzXltTU3PeWllZmZ544onQDwogLEx/ileQPV6e2PZnQ57YGBVkj+/W+wSDQV177bXyeDy67bbbQjkigDAyfQaVk5EkSW2f4iV6PSrIHt+23lVer1effvppOEYEEEamAyWdjVR3gwSgbzC9xQPw3UaggD5o9erVSk5OVnx8vJYvXy7p7OVhK1eujPJk3WN+iweg+9atW6c//OEPGjFiRLRH6RHOoIA+ZtGiRTpy5IjuuOMOrVq1So8++uh5z5k+fbqeeOIJ+Xw+JScna/fu3brnnns0btw4Pf3001GY+sIIFNDHrF+/XomJifrTn/500S8lX3bZZfL7/Vq0aJFmzZqltWvXqqamRq+99pq+/vrrCE787djiAX3EP15Y/78nT+v3ez+/6PPvvvtuSVJaWppSUlI0fPhwSdLYsWN17NgxDR06NOwzd4ZAAX1Axwvrm1tdPf/uPt0x+MS3vmbAgAGSpH79+rX9+dzj5ubm8A7cRWzxgD6g44X1knS6qUXbai5+FmUdgQL6gI4X1p9zoqEpwpOEluO6bpef7PP5XL/fH8ZxAFyKKcvfP+/CeklK8nr0YeGMKEzUPY7jlLuu6+u4zhkU0AeE6sJ6a/glOdAHhOrCemsIFNBH9MUL69niATCLQAEwi0ABMItAATCLQAEwi0ABMItAATCLQAEwq9NAOY6z0HEcv+M4/rq6ukjMBACSuhAo13WLXNf1ua7rS0hIiMRMACCJLR4AwwgUALMIFACzCBQAswgUALMIFACzCBQAswgUALMIFACzCBQAswgUALMIFACzCBQAswgUALMIFACzCBQAswgUALMIFACzCBQAswgUALMIFEyora1VampqtMeAMQQKgFkECpfk+eef1/jx4zV16lT94Ac/0MqVK1VZWanJkyfruuuu0+zZs3XixAlJ+tb18vJypaenKz09XWvXro3m4cAoAoVu2717t95++21VVVVp27Zt8vv9kqT58+frxRdf1N69e5WWlqalS5dedD0vL09r1qxRVVVV1I4FtvWP9gDoHUorAlqx/aCOBxulmt/rxhv/WZdffrkuv/xy3XXXXfrb3/6mYDCoW2+9VZL0wAMP6L777tPJkycvuB4MBhUMBpWVlSVJuv/++7Vt27aoHR9s4gwKnSqtCGjJlmoFgo1yJZ1sbNIfD3yp0opAtEdDH0eg0KkV2w+qsaml7fGAEcn666cf68Wt1aqvr9fWrVs1aNAgxcfHa+fOnZKkN998U7feequuuOKKC657vV55vV6VlZVJkkpKSiJ/YDCPLR46dTzY2O7xgOHXynPNjfKvekh3vDdWaWlpuuKKK/T6669r0aJFamho0NixY1VcXCxJ37peXFys/Px8OY6jmTNnRvy4YJ/jum6Xn+zz+dxzvxDFd8eU5e8r0CFSrX9v1NXfG6L/+pfJysrKUlFRka6//vooTYjeznGcctd1fR3X2eKhUwXZ4+WJjWm3dvK9tTpe/Jiuv/56zZkzhzghLNjioVM5GUmS1PYpXqLXo5fe3NC2DoQLgUKX5GQkESREHFs8AGYRKABmESgAZhEoAGYRKABmESgAZhEoAGZ161IXx3HqJB0N3zg9MkzSV9EeIsQ4pt6BY+q5Ua7rJnRc7FagLHMcx3+ha3l6M46pd+CYwoctHgCzCBQAs/pSoIqiPUAYcEy9A8cUJn3md1AA+p6+dAYFoI8hUADMIlAAzCJQAMwiUADM+j9zUY2mYtblSgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "vocabs = model.wv.vocab.keys()\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init=\"pca\", n_iter=5000, random_state=23)\n",
    "vectors_tsne = tsne_model.fit_transform(model[vocabs])\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.scatter(vectors_tsne[:, 0], vectors_tsne[:, 1])\n",
    "for i, word in enumerate(list(vocabs)):\n",
    "    plt.annotate(word, xy=(vectors_tsne[i, 0], vectors_tsne[i, 1]))\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題5】コーパスの前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コーパスの前処理として、特殊文字（!など）やURLの除去、大文字の小文字化といったことを行なってください。また、単語（トークン）はリストで分割してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google検索：IMDB Dataset preprocessing kaggle\n",
    "\n",
    "参考kaggleノートブック：[kernela0bd786339](https://www.kaggle.com/natlee/sentiment-analysis-of-imdb-50k-with-keras-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/takahiromotoki/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text.\n",
    "def cleanText(text, removeStopwords=True, performStemming=True):\n",
    "    \n",
    "    #regex for removing non-alphanumeric characters and spaces\n",
    "    remove_special_char = re.compile('r[^a-z\\d]', re.IGNORECASE)\n",
    "    #regex to replace all numerics\n",
    "    replace_numerics = re.compile(r'\\d+', re.IGNORECASE)\n",
    "    text = remove_special_char.sub('', text)\n",
    "    text = replace_numerics.sub('', text)\n",
    "\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    stemmer = SnowballStemmer('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    #convert text to lowercase.\n",
    "    text = text.lower().split()\n",
    "\n",
    "    \n",
    "    processedText = list()\n",
    "    for word in text:        \n",
    "        if removeStopwords:\n",
    "            if word in stop_words:\n",
    "                continue\n",
    "        if performStemming:\n",
    "            word = stemmer.stem(word)\n",
    "            \n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        word = lemmatizer.lemmatize(word, 'v')\n",
    "            \n",
    "        processedText.append(word)\n",
    "\n",
    "    #text = ' '.join(processedText)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X\n",
    "x_train_clean = [cleanText(text) for text in x_train]\n",
    "x_test_clean =  [cleanText(text) for text in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train_clean), len(x_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.<br /><br />It is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. In terms of explaining the motives and actions of the two young suicide/murderers it is better than 'Elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see. <br /><br />Flawed but honest with a terrible honesty.\""
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zero',\n",
       " 'day',\n",
       " 'leads',\n",
       " 'you',\n",
       " 'to',\n",
       " 'think,',\n",
       " 'even',\n",
       " 're-think',\n",
       " 'why',\n",
       " 'two',\n",
       " 'boys/young',\n",
       " 'men',\n",
       " 'would',\n",
       " 'do',\n",
       " 'what',\n",
       " 'they',\n",
       " 'did',\n",
       " '-',\n",
       " 'commit',\n",
       " 'mutual',\n",
       " 'suicide',\n",
       " 'via',\n",
       " 'slaughtering',\n",
       " 'theiclassmates.',\n",
       " 'it',\n",
       " 'captures',\n",
       " 'what',\n",
       " 'must',\n",
       " 'be',\n",
       " 'beyond',\n",
       " 'a',\n",
       " 'bizarre',\n",
       " 'mode',\n",
       " 'of',\n",
       " 'being',\n",
       " 'fotwo',\n",
       " 'humans',\n",
       " 'who',\n",
       " 'have',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'withdraw',\n",
       " 'from',\n",
       " 'common',\n",
       " 'civility',\n",
       " 'in',\n",
       " 'ordeto',\n",
       " 'define',\n",
       " 'theiown/mutual',\n",
       " 'world',\n",
       " 'via',\n",
       " 'coupled',\n",
       " 'destruction.<b/><b/>it',\n",
       " 'is',\n",
       " 'not',\n",
       " 'a',\n",
       " 'perfect',\n",
       " 'movie',\n",
       " 'but',\n",
       " 'given',\n",
       " 'what',\n",
       " 'money/time',\n",
       " 'the',\n",
       " 'filmmakeand',\n",
       " 'actors',\n",
       " 'had',\n",
       " '-',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " 'remarkable',\n",
       " 'product.',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'explaining',\n",
       " 'the',\n",
       " 'motives',\n",
       " 'and',\n",
       " 'actions',\n",
       " 'of',\n",
       " 'the',\n",
       " 'two',\n",
       " 'young',\n",
       " 'suicide/murderers',\n",
       " 'it',\n",
       " 'is',\n",
       " 'bettethan',\n",
       " \"'elephant'\",\n",
       " '-',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'being',\n",
       " 'a',\n",
       " 'film',\n",
       " 'that',\n",
       " 'gets',\n",
       " \"undeou'rationalistic'\",\n",
       " 'skin',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " 'fa',\n",
       " 'fabettefilm',\n",
       " 'than',\n",
       " 'almost',\n",
       " 'anything',\n",
       " 'you',\n",
       " 'are',\n",
       " 'likely',\n",
       " 'to',\n",
       " 'see.',\n",
       " '<b/><b/>flawed',\n",
       " 'but',\n",
       " 'honest',\n",
       " 'with',\n",
       " 'a',\n",
       " 'terrible',\n",
       " 'honesty.']"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題6】Word2Vecの学習 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vecの学習を行なってください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考kaggleノートブック：[Gensim Word2Vec Tutorial](https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time  # To time our operations\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = x_train_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(min_count=1, window=2, size=100, sample=6e-5, \n",
    "                     alpha=0.03, min_alpha=0.0007, negative=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.89 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=1000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 3.48 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題7】（アドバンス課題）ベクトルの可視化 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得られたベクトルをt-SNEにより可視化してください。また、いくつかの単語を選びwv.most_similarを用いて似ている単語を調べてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 0.9323524236679077),\n",
       " ('this', 0.8921775817871094),\n",
       " ('it', 0.8833895921707153),\n",
       " ('movie,', 0.8704927563667297),\n",
       " ('movie.', 0.798641562461853),\n",
       " ('film,', 0.7741544842720032),\n",
       " ('just', 0.7535284161567688),\n",
       " ('that', 0.7435012459754944),\n",
       " ('film.', 0.7337623238563538),\n",
       " ('really', 0.7329508662223816)]"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"movie\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('love.', 0.6671126484870911),\n",
       " ('love!', 0.6011080145835876),\n",
       " ('love,', 0.5934217572212219),\n",
       " ('hate', 0.5748674273490906),\n",
       " ('madly', 0.5718035697937012),\n",
       " ('friendship.', 0.5616278052330017),\n",
       " ('asleep.', 0.553730309009552),\n",
       " ('loyalty', 0.5305621027946472),\n",
       " ('loved', 0.5271797180175781),\n",
       " ('asleep,', 0.5225468873977661)]"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"love\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dog.', 0.5274533033370972),\n",
       " ('rabbit', 0.521770179271698),\n",
       " ('script.<b/><b/>first', 0.5175422430038452),\n",
       " ('\"dog', 0.5170609951019287),\n",
       " ('dog,', 0.5131518244743347),\n",
       " ('shark', 0.5075615048408508),\n",
       " ('bite', 0.4977995753288269),\n",
       " ('animal', 0.49518516659736633),\n",
       " ('cat', 0.49002036452293396),\n",
       " ('cat.', 0.48570504784584045)]"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"dog\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題8】（アドバンス課題）Word2Vecを用いた映画レビューの分類 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "262px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
